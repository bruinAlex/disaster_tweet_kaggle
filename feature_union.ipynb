{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Plan\n",
    "\n",
    "Based on previous data exploration, we'll start with the following:\n",
    "- Drop location\n",
    "- Convert keyword to a categorical\n",
    "- Vectorize tweet text using TF-IDF\n",
    "- Create categorical indicators from the text:\n",
    "    - all capitalized\n",
    "    - all lowercased\n",
    "    - contains hashtags\n",
    "    - contains a date\n",
    "    - contains link\n",
    "    - contains timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import regex as re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = English() # This does not include certain features like lemmatization!\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # includes more features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create matcher for hashtags\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text feature pipeline\n",
    "- Takes in the tweet text and returns various meta features about it\n",
    "- Does not tokenize or encode the text itself (taken care of in a separate pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self, use_count_hashtags=True, use_count_user_handles=True):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[ï¼ƒ#]{1}(\\w+)\", re.UNICODE)\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        self.use_count_hashtags = use_count_hashtags\n",
    "        self.use_count_user_handles = use_count_user_handles\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    def count_user_handles(self, obj):\n",
    "        user_handle_count = len(re.findall(self.user_handle_pattern, obj))\n",
    "        return user_handle_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalLemmatizedKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "        \n",
    "    def spacy_lemmatizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # There should only be one keyword (not removing %20 spaces)\n",
    "        if len(doc) > 1:\n",
    "            print('More than one token found; expecting single token')\n",
    "            \n",
    "        return doc[0].lemma_\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "\n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalRawKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "#         # Instantiate OneHotEncoder\n",
    "#         ohe = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "#                                          drop='first',\n",
    "#                                          sparse=False) \n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, pca=False, pca_n = None, remove_hashtag=True, remove_user_handle=True, remove_stop_words=True):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.pca_n = pca_n\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "        self.remove_user_handle = remove_user_handle\n",
    "#         self.user_handle_matcher = Matcher(nlp.vocab)\n",
    "#         self.user_handle_matcher.add('HANDLE', None, [{'ORTH': ')'}, {'IS_ASCII': True}])\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))(@[A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "            \n",
    "        # Create a list of user handles\n",
    "        user_handles = re.findall(self.user_handle_pattern, doc.text)      \n",
    "        \n",
    "        # Convert spacy tokens to a list of string tokens\n",
    "        token_list = [t.text.lower() for t in doc if not t.is_punct | t.is_space]\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            token_list = [t for t in token_list if t not in stop_words]\n",
    "        \n",
    "        if self.remove_user_handle:\n",
    "            token_list = [t for t in token_list if t not in user_handles]\n",
    "        \n",
    "        if self.remove_hashtag:\n",
    "            token_list = [t.replace(\"#\", \"\") for t in token_list]\n",
    "            \n",
    "        return token_list\n",
    "        \n",
    "#         if self.remove_hashtag:\n",
    "#             # Lower cases text and strips the hash symbol from hashtag while leaving rest of tag\n",
    "#             return [t.text.lower().replace(\"#\", \"\") for t in doc if t.text.lower() not in stop_words and not t.is_punct | t.is_space] \n",
    "#         else:\n",
    "#             # Lower cases text but keeps hash symbol in hashtag\n",
    "#             return [t.text.lower() for t in doc if t.text.lower() not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "#         X = super().transform(raw_documents, copy=copy)\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "#         df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "#         return df\n",
    "#         return X.toarray()\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "#         X = super().fit_transform(raw_documents, y=y)\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "#         df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "#         return df\n",
    "#         return X.toarray()\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.pca_n)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "            \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that tokenizes text\n",
    "# class TextTokenizerTransformer(BaseEstimator, TransformerMixin):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "        \n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "    \n",
    "#     def spacy_tokenizer(self, obj):\n",
    "#         doc = nlp(obj)\n",
    "\n",
    "#         # Looks for hashtags\n",
    "#         matches = matcher(doc)\n",
    "#         spans = []\n",
    "#         for match_id, start, end in matches:\n",
    "#             spans.append(doc[start:end])\n",
    "\n",
    "#         for span in spans:\n",
    "#             span.merge()\n",
    "\n",
    "#         return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         X['tokens'] = X['text'].apply(self.spacy_tokenizer)\n",
    "        \n",
    "#         X['tokens'] = \" \".join(X['tokens'])\n",
    "#         return X['tokens']\n",
    "    \n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def fit_transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         X['tokens'] = X['text'].apply(self.spacy_tokenizer)\n",
    "        \n",
    "#         X['tokens'] = \" \".join(X['tokens'])\n",
    "#         return X['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that takes in a string and returns some features\n",
    "# class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "    \n",
    "#     def spacy_tokenizer(self, obj):\n",
    "#         doc = nlp(obj)\n",
    "\n",
    "#         # Looks for hashtags\n",
    "#         matches = matcher(doc)\n",
    "#         spans = []\n",
    "#         for match_id, start, end in matches:\n",
    "#             spans.append(doc[start:end])\n",
    "\n",
    "#         for span in spans:\n",
    "#             span.merge()\n",
    "\n",
    "#         return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         # Embed text as a bag of words using tfidf\n",
    "#         tfidf = TfidfVectorizer(tokenizer = self.spacy_tokenizer)\n",
    "#         X = tfidf.fit_transform(X['text'])\n",
    "        \n",
    "#         # returns numpy array\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_raw_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalRawKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the lemmatized keyword categorical pipeline\n",
    "cat_lemma_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalLemmatizedKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer(pca=True, pca_n = 50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# #Defining the steps in the categorical pipeline \n",
    "# categorical_pipeline = Pipeline( steps = [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n",
    "                                  \n",
    "#                                   ( 'cat_transformer', CategoricalTransformer() ), \n",
    "                                  \n",
    "#                                   ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "    \n",
    "# #Defining the steps in the numerical pipeline     \n",
    "# numerical_pipeline = Pipeline( steps = [ ( 'num_selector', FeatureSelector(numerical_features) ),\n",
    "                                  \n",
    "#                                   ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  \n",
    "#                                   ('imputer', SimpleImputer(strategy = 'median') ),\n",
    "                                  \n",
    "#                                   ( 'std_scaler', StandardScaler() ) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_raw_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the full pipeline with the training and the model\n",
    "- The data transformation part of the pipeline that does TFIDF will return different number of features based on the data fed in\n",
    "\n",
    "# Solution: Separate the feature pipeline with the model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training time: tfidf.fit_transform(X_train)\n",
    "inference: tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PCA'd (50 dims) training and test sets for the lemma and raw pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pipeline, name in zip([full_raw_keyword_pca_50_pipeline, full_lemma_keyword_pca_50_pipeline], ['full_raw_keyword_pca_50_pipeline', 'full_lemma_keyword_pca_50_pipeline']):\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    np.save(name + '_X_train', X_train_processed)\n",
    "    np.save(name + '_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_lemma_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_lemma_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "np.save('raw_keyword_categorical_X_train', X_train_processed)\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "np.save('raw_keyword_categorical_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "np.save('lemma_keyword_categorical_X_train_csr', sparse.csr_matrix(X_train_processed))\n",
    "np.save('lemma_keyword_categorical_y_train', y_train)\n",
    "# np.save('raw_keyword_categorical_y_train_csr', sparse.csr_matrix(y_train)) # Don't save as a sparse matrix, else you will need to reshape it later for training\n",
    "np.save('lemma_keyword_categorical_test_processed_csr', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed data\n",
    "- Save the output of the transform pipelines to save memory\n",
    "- Can save it raw (very large)\n",
    "- Or save as a sparse matrix\n",
    "- Do not save the target labels as a sparse as you'll have to reshape it from (1, n) to (n, ) later, and the space savings is probably very small\n",
    "- Note that there were some problems reading the sparse matrix into some sklearn models at training\n",
    "    - Will need to look into this problem more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000, # Try 4000...\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The full pipeline as a step in another pipeline with an estimator as the final step\n",
    "# full_pipeline_m = Pipeline(steps = [\n",
    "#     ('full_pipeline', full_pipeline),\n",
    "#     ('model', LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)) \n",
    "# ])\n",
    "\n",
    "# #Can call fit on it just like any other pipeline\n",
    "# full_pipeline_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df.copy().sample(1000, random_state=42)\n",
    "y_test = X_test.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test_processed = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict\n",
    "predicted = lrcv.predict(X_test_processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline_m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with tfidf, upper, lower text\n",
    "Logistic Regression Accuracy: 0.851  \n",
    "Logistic Regression Precision: 0.9287925696594427  \n",
    "Logistic Regression Recall: 0.704225352112676  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same as above but using the keyword column as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bumped up the LRCV iterations to 4000 due to non-convergence at iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upped cv to 10 using additional feature of num_hashtags in text\n",
    "# Model training takes under 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, vals in lrcv.scores_.items():\n",
    "    for idx, val in enumerate(vals):\n",
    "        print(idx)\n",
    "        print(val)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lrcv.predict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
