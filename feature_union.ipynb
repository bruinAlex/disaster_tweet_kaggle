{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns some features\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "#     # Helper function to extract year from column 'dates' \n",
    "#     def get_year( self, obj ):\n",
    "#         return str(obj)[:4]\n",
    "    \n",
    "#     # Helper function to extract month from column 'dates'\n",
    "#     def get_month( self, obj ):\n",
    "#         return str(obj)[4:6]\n",
    "    \n",
    "#     # Helper function to extract day from column 'dates'\n",
    "#     def get_day(self, obj):\n",
    "#         return str(obj)[6:8]\n",
    "    \n",
    "#     # Helper function that converts values to Binary depending on input \n",
    "#     def create_binary(self, obj):\n",
    "#         if obj == 0:\n",
    "#             return 'No'\n",
    "#         else:\n",
    "#             return 'Yes'\n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def lower_text(self, obj):\n",
    "        return obj.lower()\n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "#         # Depending on constructor argument break dates column into specified units\n",
    "#         # using the helper functions written above \n",
    "#         for spec in self._use_dates:\n",
    "#             exec( \"X.loc[:,'{}'] = X['date'].apply(self.get_{})\".format( spec, spec ) )\n",
    "        \n",
    "#         # Drop unusable column \n",
    "#         X = X.drop('date', axis = 1 )\n",
    "\n",
    "#         # Convert these columns to binary for one-hot-encoding later\n",
    "#         X.loc[:,'waterfront'] = X['waterfront'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'view'] = X['view'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'yr_renovated'] = X['yr_renovated'].apply( self.create_binary )\n",
    "        \n",
    "        # Test function should return a new col called 'lower' that has the same text as the tweet text, but all lowercase\n",
    "        X.loc[:, 'lower'] = X['text'].apply(self.lower_text)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that breaks dates column into year, month and day into separate columns and\n",
    "# # converts certain features to binary \n",
    "# class CategoricalTransformer( BaseEstimator, TransformerMixin ):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self, use_dates = ['year', 'month', 'day'] ):\n",
    "#         self._use_dates = use_dates\n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit( self, X, y = None  ):\n",
    "#         return self\n",
    "\n",
    "#     # Helper function to extract year from column 'dates' \n",
    "#     def get_year( self, obj ):\n",
    "#         return str(obj)[:4]\n",
    "    \n",
    "#     # Helper function to extract month from column 'dates'\n",
    "#     def get_month( self, obj ):\n",
    "#         return str(obj)[4:6]\n",
    "    \n",
    "#     # Helper function to extract day from column 'dates'\n",
    "#     def get_day(self, obj):\n",
    "#         return str(obj)[6:8]\n",
    "    \n",
    "#     # Helper function that converts values to Binary depending on input \n",
    "#     def create_binary(self, obj):\n",
    "#         if obj == 0:\n",
    "#             return 'No'\n",
    "#         else:\n",
    "#             return 'Yes'\n",
    "    \n",
    "#     # Transformer method we wrote for this transformer \n",
    "#     def transform(self, X , y = None ):\n",
    "#         # Depending on constructor argument break dates column into specified units\n",
    "#         #using the helper functions written above \n",
    "#         for spec in self._use_dates:\n",
    "\n",
    "#         exec( \"X.loc[:,'{}'] = X['date'].apply(self.get_{})\".format( spec, spec ) )\n",
    "#         # Drop unusable column \n",
    "#         X = X.drop('date', axis = 1 )\n",
    "\n",
    "#         # Convert these columns to binary for one-hot-encoding later\n",
    "#         X.loc[:,'waterfront'] = X['waterfront'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'view'] = X['view'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'yr_renovated'] = X['yr_renovated'].apply( self.create_binary )\n",
    "#         # returns numpy array\n",
    "#         return X.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categrical features to pass down the categorical pipeline \n",
    "# categorical_features = ['date', 'waterfront', 'view', 'yr_renovated']\n",
    "\n",
    "# #Numerical features to pass down the numerical pipeline \n",
    "# numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "#                 'condition', 'grade', 'sqft_basement', 'yr_built']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# #Defining the steps in the categorical pipeline \n",
    "# categorical_pipeline = Pipeline( steps = [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n",
    "                                  \n",
    "#                                   ( 'cat_transformer', CategoricalTransformer() ), \n",
    "                                  \n",
    "#                                   ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "    \n",
    "# #Defining the steps in the numerical pipeline     \n",
    "# numerical_pipeline = Pipeline( steps = [ ( 'num_selector', FeatureSelector(numerical_features) ),\n",
    "                                  \n",
    "#                                   ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  \n",
    "#                                   ('imputer', SimpleImputer(strategy = 'median') ),\n",
    "                                  \n",
    "#                                   ( 'std_scaler', StandardScaler() ) ] )\n",
    "\n",
    "# Define the text pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_transformer', TextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[('text_pipeline', text_pipeline)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['our deeds are the reason of this #earthquake may allah forgive us all'],\n",
       "       ['forest fire near la ronge sask. canada'],\n",
       "       [\"all residents asked to 'shelter in place' are being notified by officers. no other evacuation or shelter in place orders are expected\"],\n",
       "       ...,\n",
       "       ['m1.94 [01:04 utc]?5km s of volcano hawaii. http://t.co/zdtoyd8ebj'],\n",
       "       ['police investigating after an e-bike collided with a car in little portugal. e-bike rider suffered serious non-life threatening injuries.'],\n",
       "       ['the latest: more homes razed by northern california wildfire - abc news http://t.co/ymy4rskq3d']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(string):\n",
    "    doc = nlp(string)\n",
    "#     return doc.text.split() # Disregards puncutation and contractions\n",
    "#     return [token.orth_ for token in doc] # This should work better, but isn't perfect. This also splits hashtags hash symbol and the word tag\n",
    "#     return [token.orth_ for token in doc if not token.is_punct | token.is_space]\n",
    "\n",
    "    # Looks for hashtags\n",
    "    matches = matcher(doc)\n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        spans.append(doc[start:end])\n",
    "\n",
    "\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "#     print([t.text for t in doc])\n",
    "#     return [t.text for t in doc if not t.is_punct | t.is_space]\n",
    "\n",
    "#     return string.isupper(), string.islower(), [t.text for t in doc if not t.is_punct | t.is_space]\n",
    "    return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer_dataframe(df):\n",
    "    \"\"\"\n",
    "    Works on a dataframe rather than directly on the strings\n",
    "    Can't use this one for the sklearn pipeline\n",
    "    \"\"\"\n",
    "    doc = nlp(df['text'])\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        spans.append(doc[start:end])\n",
    "\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    df['upper'] = df['text'].isupper()\n",
    "    df['lower'] = df['text'].islower()\n",
    "    df['token_list'] = [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_tokenizer(\"This is a   sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Some text is going down the street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Some text is going down the street"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_py_tokens',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_disk',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'merge',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'print_tree',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Some', 'text', 'is', 'going', 'down', 'the', 'street']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for el in doc.vocab:\n",
    "#     print(el.text)\n",
    "\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk)\n",
    "\n",
    "# for sent in doc.sents:\n",
    "#     print(sent)\n",
    "\n",
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.head(30).apply(spacy_tokenizer, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for el in train_df.head(30)['token_list']:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "\n",
    "doc = nlp(\"this is an #example of an awesome tweet with #manyhashtags #amazing\")\n",
    "matches = matcher(doc)\n",
    "spans = []\n",
    "for match_id, start, end in matches:\n",
    "    spans.append(doc[start:end])\n",
    "    \n",
    "    \n",
    "for span in spans:\n",
    "    span.merge()\n",
    "\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "# x = v.fit_transform(df['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf.fit_transform(train_df['text'].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.todense()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], test_size=0.3, random_state=42)\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "\n",
    "# # Create pipeline using Bag of Words\n",
    "# pipe = Pipeline([\n",
    "#                 ('vectorizer', tfidf),\n",
    "#                  ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1).fit(train_df['text'], train_df['target'])\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([\n",
    "                ('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model generation\n",
    "pipe.fit(train_df['text'],train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wow, 85% cross-validation accuracy with almost no feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
