{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Plan\n",
    "\n",
    "Based on previous data exploration, we'll start with the following:\n",
    "- Drop location\n",
    "- Convert keyword to a categorical\n",
    "- Vectorize tweet text using TF-IDF\n",
    "- Create categorical indicators from the text:\n",
    "    - all capitalized\n",
    "    - all lowercased\n",
    "    - contains hashtags\n",
    "    - contains a date\n",
    "    - contains link\n",
    "    - contains timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import regex as re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = English() # This does not include certain features like lemmatization!\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # includes more features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matcher for hashtags\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text feature pipeline\n",
    "- Takes in the tweet text and returns various meta features about it\n",
    "- Does not tokenize or encode the text itself (taken care of in a separate pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalLemmatizedKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "        \n",
    "    def spacy_lemmatizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # There should only be one keyword (not removing %20 spaces)\n",
    "        if len(doc) > 1:\n",
    "            print('More than one token found; expecting single token')\n",
    "            \n",
    "        return doc[0].lemma_\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "\n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalRawKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "#         # Instantiate OneHotEncoder\n",
    "#         ohe = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "#                                          drop='first',\n",
    "#                                          sparse=False) \n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, pca=False, pca_n = None, remove_hashtag=True):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.pca_n = pca_n\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "        if self.remove_hashtag:\n",
    "            # Lower cases text and strips the hash symbol from hashtag while leaving rest of tag\n",
    "            return [t.text.lower().replace(\"#\", \"\") for t in doc if t.text.lower() not in stop_words and not t.is_punct | t.is_space] \n",
    "        else:\n",
    "            # Lower cases text but keeps hash symbol in hashtag\n",
    "            return [t.text.lower() for t in doc if t.text.lower() not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "#         X = super().transform(raw_documents, copy=copy)\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "#         df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "#         return df\n",
    "#         return X.toarray()\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "#         X = super().fit_transform(raw_documents, y=y)\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "#         df = pd.DataFrame(X.toarray(), columns=self.get_feature_names())\n",
    "#         return df\n",
    "#         return X.toarray()\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.pca_n)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "            \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenizer = DenseTfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeds', 'reason', 'earthquake', 'allah', 'forgive']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenizer.spacy_tokenizer(train_df.loc[0, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5469654998740872"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['text'].str.contains(\"http\")]['target'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/spacy/lib/python3.7/site-packages/pandas/core/strings.py:1952: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>@PhDSquares #mufc they've built so much hype a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>63</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>78</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Abuja</td>\n",
       "      <td>Noches El-Bestia '@Alexis_Sanchez: happy to se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>91</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Concord, CA</td>\n",
       "      <td>@Navista7 Steve these fires out here are somet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>10829</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <td>10830</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@jt_ruff23 @cameronhacker and I wrecked you both</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>10851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @LivingSafely: #NWS issues Severe #Thunders...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1950 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                location  \\\n",
       "31       48   ablaze              Birmingham   \n",
       "36       54   ablaze                Pretoria   \n",
       "43       63   ablaze                     NaN   \n",
       "54       78   ablaze                   Abuja   \n",
       "63       91   ablaze             Concord, CA   \n",
       "...     ...      ...                     ...   \n",
       "7577  10829  wrecked  #NewcastleuponTyne #UK   \n",
       "7578  10830  wrecked                     NaN   \n",
       "7581  10833  wrecked                 Lincoln   \n",
       "7596  10851      NaN                     NaN   \n",
       "7609  10870      NaN                     NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "36    @PhDSquares #mufc they've built so much hype a...       0  \n",
       "43         SOOOO PUMPED FOR ABLAZE ???? @southridgelife       0  \n",
       "54    Noches El-Bestia '@Alexis_Sanchez: happy to se...       0  \n",
       "63    @Navista7 Steve these fires out here are somet...       1  \n",
       "...                                                 ...     ...  \n",
       "7577  @widda16 ... He's gone. You can relax. I thoug...       0  \n",
       "7578   @jt_ruff23 @cameronhacker and I wrecked you both       0  \n",
       "7581  @engineshed Great atmosphere at the British Li...       0  \n",
       "7596  RT @LivingSafely: #NWS issues Severe #Thunders...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "\n",
       "[1950 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['text'].str.contains(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <th>0</th>\n",
       "      <td>bbcmtd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <th>0</th>\n",
       "      <td>PhDSquares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <th>0</th>\n",
       "      <td>southridgelife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <th>0</th>\n",
       "      <td>Alexis_Sanchez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <th>0</th>\n",
       "      <td>Navista7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7578</th>\n",
       "      <th>1</th>\n",
       "      <td>cameronhacker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <th>0</th>\n",
       "      <td>engineshed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <th>0</th>\n",
       "      <td>LivingSafely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7609</th>\n",
       "      <th>0</th>\n",
       "      <td>aria_ahrary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheTawniest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2608 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0\n",
       "     match                \n",
       "31   0              bbcmtd\n",
       "36   0          PhDSquares\n",
       "43   0      southridgelife\n",
       "54   0      Alexis_Sanchez\n",
       "63   0            Navista7\n",
       "...                    ...\n",
       "7578 1       cameronhacker\n",
       "7581 0          engineshed\n",
       "7596 0        LivingSafely\n",
       "7609 0         aria_ahrary\n",
       "     1         TheTawniest\n",
       "\n",
       "[2608 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'].str.extractall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-fef8760fd522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7578\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'str'"
     ]
    }
   ],
   "source": [
    "train_df.loc[7578, 'text'].str.extractall(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@jt_ruff23 @cameronhacker and I wrecked you both'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[7578, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that tokenizes text\n",
    "# class TextTokenizerTransformer(BaseEstimator, TransformerMixin):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "        \n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "    \n",
    "#     def spacy_tokenizer(self, obj):\n",
    "#         doc = nlp(obj)\n",
    "\n",
    "#         # Looks for hashtags\n",
    "#         matches = matcher(doc)\n",
    "#         spans = []\n",
    "#         for match_id, start, end in matches:\n",
    "#             spans.append(doc[start:end])\n",
    "\n",
    "#         for span in spans:\n",
    "#             span.merge()\n",
    "\n",
    "#         return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         X['tokens'] = X['text'].apply(self.spacy_tokenizer)\n",
    "        \n",
    "#         X['tokens'] = \" \".join(X['tokens'])\n",
    "#         return X['tokens']\n",
    "    \n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def fit_transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         X['tokens'] = X['text'].apply(self.spacy_tokenizer)\n",
    "        \n",
    "#         X['tokens'] = \" \".join(X['tokens'])\n",
    "#         return X['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that takes in a string and returns some features\n",
    "# class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit(self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "    \n",
    "#     def spacy_tokenizer(self, obj):\n",
    "#         doc = nlp(obj)\n",
    "\n",
    "#         # Looks for hashtags\n",
    "#         matches = matcher(doc)\n",
    "#         spans = []\n",
    "#         for match_id, start, end in matches:\n",
    "#             spans.append(doc[start:end])\n",
    "\n",
    "#         for span in spans:\n",
    "#             span.merge()\n",
    "\n",
    "#         return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "#     # Transformer method to take in strings from a dataframe and return some extra features\n",
    "#     def transform(self, X , y = None):\n",
    "#         # Copy the incoming df to prevent setting on copy errors\n",
    "#         X = X.copy()\n",
    "        \n",
    "#         # Embed text as a bag of words using tfidf\n",
    "#         tfidf = TfidfVectorizer(tokenizer = self.spacy_tokenizer)\n",
    "#         X = tfidf.fit_transform(X['text'])\n",
    "        \n",
    "#         # returns numpy array\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_raw_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalRawKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the lemmatized keyword categorical pipeline\n",
    "cat_lemma_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalLemmatizedKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer(pca=True, pca_n = 50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# #Defining the steps in the categorical pipeline \n",
    "# categorical_pipeline = Pipeline( steps = [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n",
    "                                  \n",
    "#                                   ( 'cat_transformer', CategoricalTransformer() ), \n",
    "                                  \n",
    "#                                   ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "    \n",
    "# #Defining the steps in the numerical pipeline     \n",
    "# numerical_pipeline = Pipeline( steps = [ ( 'num_selector', FeatureSelector(numerical_features) ),\n",
    "                                  \n",
    "#                                   ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  \n",
    "#                                   ('imputer', SimpleImputer(strategy = 'median') ),\n",
    "                                  \n",
    "#                                   ( 'std_scaler', StandardScaler() ) ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_raw_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the full pipeline with the training and the model\n",
    "- The data transformation part of the pipeline that does TFIDF will return different number of features based on the data fed in\n",
    "\n",
    "# Solution: Separate the feature pipeline with the model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training time: tfidf.fit_transform(X_train)\n",
    "inference: tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PCA'd (50 dims) training and test sets for the lemma and raw pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 3.2min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total= 1.3min\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 3.1min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 11min 8s, sys: 7.88 s, total: 11min 15s\n",
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for pipeline, name in zip([full_raw_keyword_pca_50_pipeline, full_lemma_keyword_pca_50_pipeline], ['full_raw_keyword_pca_50_pipeline', 'full_lemma_keyword_pca_50_pipeline']):\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    np.save(name + '_X_train', X_train_processed)\n",
    "    np.save(name + '_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total= 1.3min\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 3.0min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 4min 15s, sys: 1.79 s, total: 4min 17s\n",
      "Wall time: 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_lemma_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 50s, sys: 683 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_lemma_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "np.save('raw_keyword_categorical_X_train', X_train_processed)\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "np.save('raw_keyword_categorical_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "np.save('lemma_keyword_categorical_X_train_csr', sparse.csr_matrix(X_train_processed))\n",
    "np.save('lemma_keyword_categorical_y_train', y_train)\n",
    "# np.save('raw_keyword_categorical_y_train_csr', sparse.csr_matrix(y_train)) # Don't save as a sparse matrix, else you will need to reshape it later for training\n",
    "np.save('lemma_keyword_categorical_test_processed_csr', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed data\n",
    "- Save the output of the transform pipelines to save memory\n",
    "- Can save it raw (very large)\n",
    "- Or save as a sparse matrix\n",
    "- Do not save the target labels as a sparse as you'll have to reshape it from (1, n) to (n, ) later, and the space savings is probably very small\n",
    "- Note that there were some problems reading the sparse matrix into some sklearn models at training\n",
    "    - Will need to look into this problem more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 17s, sys: 31 s, total: 8min 48s\n",
      "Wall time: 38min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=10, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=4000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000, # Try 4000...\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The full pipeline as a step in another pipeline with an estimator as the final step\n",
    "# full_pipeline_m = Pipeline(steps = [\n",
    "#     ('full_pipeline', full_pipeline),\n",
    "#     ('model', LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)) \n",
    "# ])\n",
    "\n",
    "# #Can call fit on it just like any other pipeline\n",
    "# full_pipeline_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df.copy().sample(1000, random_state=42)\n",
    "y_test = X_test.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test_processed = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 23447)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 23447)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.3 ms, sys: 8.12 ms, total: 66.5 ms\n",
      "Wall time: 39.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Predict\n",
    "predicted = lrcv.predict(X_test_processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline_m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.851\n",
      "Logistic Regression Precision: 0.9287925696594427\n",
      "Logistic Regression Recall: 0.704225352112676\n"
     ]
    }
   ],
   "source": [
    "# # %%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with tfidf, upper, lower text\n",
    "Logistic Regression Accuracy: 0.851  \n",
    "Logistic Regression Precision: 0.9287925696594427  \n",
    "Logistic Regression Recall: 0.704225352112676  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same as above but using the keyword column as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.925\n",
      "Logistic Regression Precision: 0.9511568123393316\n",
      "Logistic Regression Recall: 0.8685446009389671\n",
      "CPU times: user 2.08 ms, sys: 0 ns, total: 2.08 ms\n",
      "Wall time: 1.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bumped up the LRCV iterations to 4000 due to non-convergence at iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.925\n",
      "Logistic Regression Precision: 0.9511568123393316\n",
      "Logistic Regression Recall: 0.8685446009389671\n",
      "CPU times: user 2.86 ms, sys: 81 µs, total: 2.94 ms\n",
      "Wall time: 2.41 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.991\n",
      "Logistic Regression Precision: 0.995249406175772\n",
      "Logistic Regression Recall: 0.9835680751173709\n",
      "CPU times: user 4.16 ms, sys: 8 µs, total: 4.17 ms\n",
      "Wall time: 3.58 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9893742621015348"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upped cv to 10 using additional feature of num_hashtags in text\n",
    "# Model training takes under 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.998\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 0.9953051643192489\n",
      "CPU times: user 19.3 ms, sys: 4.17 ms, total: 23.4 ms\n",
      "Wall time: 616 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9976470588235293"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0.         0.02898551 0.06197183 0.23173804 0.5950096  0.63829787\n",
      " 0.61848739 0.61435726 0.61258278 0.61083744]\n",
      "\n",
      "1\n",
      "[0.         0.         0.01204819 0.13736264 0.27697842 0.26367461\n",
      " 0.27530364 0.28726287 0.28922237 0.2946794 ]\n",
      "\n",
      "2\n",
      "[0.         0.01201201 0.02292264 0.10383747 0.34726688 0.36936937\n",
      " 0.3826087  0.38205499 0.37845706 0.38205499]\n",
      "\n",
      "3\n",
      "[0.         0.01215805 0.04733728 0.08866995 0.21463415 0.2124431\n",
      " 0.22288262 0.21791045 0.22056632 0.21791045]\n",
      "\n",
      "4\n",
      "[0.         0.         0.03478261 0.11160714 0.33063209 0.38818565\n",
      " 0.40540541 0.41644562 0.42272127 0.42687747]\n",
      "\n",
      "5\n",
      "[0.         0.03003003 0.06395349 0.16091954 0.37873754 0.44216691\n",
      " 0.44219653 0.43804035 0.44189383 0.43965517]\n",
      "\n",
      "6\n",
      "[0.         0.03003003 0.05763689 0.06970509 0.33397313 0.39092496\n",
      " 0.40734558 0.44732577 0.45088567 0.45980707]\n",
      "\n",
      "7\n",
      "[0.         0.01796407 0.03529412 0.16284987 0.40618956 0.4020979\n",
      " 0.3965812  0.39255499 0.39261745 0.3919598 ]\n",
      "\n",
      "8\n",
      "[0.         0.02967359 0.03488372 0.1010101  0.5347432  0.56363636\n",
      " 0.55860349 0.55569462 0.54956085 0.55721393]\n",
      "\n",
      "9\n",
      "[0.         0.01212121 0.04166667 0.2746988  0.64385692 0.64761905\n",
      " 0.63363755 0.62158648 0.61799218 0.61357702]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, vals in lrcv.scores_.items():\n",
    "    for idx, val in enumerate(vals):\n",
    "        print(idx)\n",
    "        print(val)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43945727482425345"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.02898551, 0.06197183, 0.23173804, 0.5950096 ,\n",
       "        0.63829787, 0.61848739, 0.61435726, 0.61258278, 0.61083744],\n",
       "       [0.        , 0.        , 0.01204819, 0.13736264, 0.27697842,\n",
       "        0.26367461, 0.27530364, 0.28726287, 0.28922237, 0.2946794 ],\n",
       "       [0.        , 0.01201201, 0.02292264, 0.10383747, 0.34726688,\n",
       "        0.36936937, 0.3826087 , 0.38205499, 0.37845706, 0.38205499],\n",
       "       [0.        , 0.01215805, 0.04733728, 0.08866995, 0.21463415,\n",
       "        0.2124431 , 0.22288262, 0.21791045, 0.22056632, 0.21791045],\n",
       "       [0.        , 0.        , 0.03478261, 0.11160714, 0.33063209,\n",
       "        0.38818565, 0.40540541, 0.41644562, 0.42272127, 0.42687747],\n",
       "       [0.        , 0.03003003, 0.06395349, 0.16091954, 0.37873754,\n",
       "        0.44216691, 0.44219653, 0.43804035, 0.44189383, 0.43965517],\n",
       "       [0.        , 0.03003003, 0.05763689, 0.06970509, 0.33397313,\n",
       "        0.39092496, 0.40734558, 0.44732577, 0.45088567, 0.45980707],\n",
       "       [0.        , 0.01796407, 0.03529412, 0.16284987, 0.40618956,\n",
       "        0.4020979 , 0.3965812 , 0.39255499, 0.39261745, 0.3919598 ],\n",
       "       [0.        , 0.02967359, 0.03488372, 0.1010101 , 0.5347432 ,\n",
       "        0.56363636, 0.55860349, 0.55569462, 0.54956085, 0.55721393],\n",
       "       [0.        , 0.01212121, 0.04166667, 0.2746988 , 0.64385692,\n",
       "        0.64761905, 0.63363755, 0.62158648, 0.61799218, 0.61357702]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.01729745, 0.04124974, 0.14423986, 0.40620215,\n",
       "       0.43184158, 0.43430521, 0.43732334, 0.43764998, 0.43945727])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4012267719708273"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 s, sys: 743 ms, total: 44.6 s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lrcv.predict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
