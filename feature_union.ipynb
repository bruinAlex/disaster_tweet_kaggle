{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Plan\n",
    "\n",
    "Based on previous data exploration, we'll start with the following:\n",
    "- Drop location\n",
    "- Convert keyword to a categorical\n",
    "- Vectorize tweet text using TF-IDF\n",
    "- Create categorical indicators from the text:\n",
    "    - all capitalized\n",
    "    - all lowercased\n",
    "    - count of hashtags\n",
    "    - count of user handles\n",
    "    - contains a date\n",
    "    - contains link\n",
    "    - contains timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import regex as re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD # SparsePCA may avoid problems below\n",
    "from scipy import sparse\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = English() # This does not include certain features like lemmatization!\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # includes more features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create matcher for hashtags\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counts(text_series, return_df = True, tokenizer=Tokenizer(nlp.vocab)):\n",
    "    \"\"\"\n",
    "    Helper function to get most common tokens.\n",
    "    \"\"\"\n",
    "    token_counts = Counter()\n",
    "    for doc in tokenizer.pipe(text_series, batch_size=50):\n",
    "        for token in doc:\n",
    "            # Skip url-like\n",
    "            if token.like_url:\n",
    "                continue\n",
    "            # Skip emails\n",
    "            if token.like_email:\n",
    "                continue\n",
    "            if token.text.lower() in custom_stopwords:\n",
    "                continue\n",
    "            token_counts[token.orth_.lower()] += 1 # Equivalently, token.text\n",
    "\n",
    "    if return_df:\n",
    "        token_counts_df = pd.DataFrame.from_dict(token_counts, orient='index').reset_index().sort_values(by=0, ascending=False)\n",
    "        return token_counts_df\n",
    "    else:\n",
    "        return token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text feature pipeline\n",
    "- Takes in the tweet text and returns various meta features about it\n",
    "- Does not tokenize or encode the text itself (taken care of in a separate pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self, use_count_hashtags=True, use_count_user_handles=True):\n",
    "        self.use_count_hashtags = use_count_hashtags\n",
    "        self.use_count_user_handles = use_count_user_handles\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@[A-Za-z0-9_]+[A-Za-z0-9-_]+\", re.UNICODE)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    def count_user_handles(self, obj):\n",
    "        user_handle_count = len(re.findall(self.user_handle_pattern, obj))\n",
    "        return user_handle_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalLemmatizedKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "        \n",
    "    def spacy_lemmatizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # There should only be one keyword (not removing %20 spaces)\n",
    "        if len(doc) > 1:\n",
    "            print('More than one token found; expecting single token')\n",
    "            \n",
    "        return doc[0].lemma_\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "\n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalRawKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "#         # Instantiate OneHotEncoder\n",
    "#         ohe = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "#                                          drop='first',\n",
    "#                                          sparse=False) \n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, pca=False, target_dim = None, remove_hashtag=True, remove_user_handle=True, remove_stop_words=True):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.target_dim = target_dim\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "        self.remove_user_handle = remove_user_handle\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))(@[A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "            \n",
    "        # Create a list of user handles\n",
    "        user_handles = re.findall(self.user_handle_pattern, doc.text)      \n",
    "        \n",
    "        # Convert spacy tokens to a list of string tokens\n",
    "        token_list = [t.text.lower() for t in doc if not t.is_punct | t.is_space]\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            token_list = [t for t in token_list if t not in stop_words]\n",
    "        \n",
    "        if self.remove_user_handle:\n",
    "            token_list = [t for t in token_list if t not in user_handles]\n",
    "        \n",
    "        if self.remove_hashtag:\n",
    "            token_list = [t.replace(\"#\", \"\") for t in token_list]\n",
    "            \n",
    "        return token_list\n",
    "\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.target_dim)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "            \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vec_method='tfidf', pca=False, target_dim = None, trunc_svd=False, remove_hashtag=True, remove_user_handle=True, remove_stop_words=True):\n",
    "        self.vec_method = vec_method\n",
    "        if self.vec_method == 'tfidf':\n",
    "            self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        elif self.vec_method == 'bow':\n",
    "            self.bow_model = CountVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.trunc_svd = trunc_svd\n",
    "        self.target_dim = target_dim\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "        self.remove_user_handle = remove_user_handle\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))(@[A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "            \n",
    "        # Create a list of user handles\n",
    "        user_handles = re.findall(self.user_handle_pattern, doc.text)      \n",
    "        \n",
    "        # Convert spacy tokens to a list of string tokens\n",
    "        token_list = [t.text.lower() for t in doc if not t.is_punct | t.is_space]\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            token_list = [t for t in token_list if t not in stop_words]\n",
    "        \n",
    "        if self.remove_user_handle:\n",
    "            token_list = [t for t in token_list if t not in user_handles]\n",
    "        \n",
    "        if self.remove_hashtag:\n",
    "            token_list = [t.replace(\"#\", \"\") for t in token_list]\n",
    "            \n",
    "        return token_list\n",
    "\n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        if self.vec_method == 'tfidf':\n",
    "            X = self.tfidf_model.transform(raw_documents['text'])\n",
    "        elif self.vec_method == 'bow':\n",
    "            X = self.bow_model.transform(raw_documents['text'])\n",
    "            \n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        # If truncated SVD\n",
    "        if self.trunc_svd:\n",
    "#             # PCA requires a dense matrix\n",
    "#             # Tf-idf returns a sparse one\n",
    "#             X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for truncated svd - unsure if needed\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Transform using trained model and return reduced array\n",
    "            X = self.trunc_svd_model.transform(X)\n",
    "\n",
    "            return X\n",
    "        \n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        if self.vec_method == 'tfidf':\n",
    "            X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "        elif self.vec_method == 'bow':\n",
    "            X = self.bow_model.fit_transform(raw_documents['text'])\n",
    "            \n",
    "        # TODO: try sklearn.truncatedSVD which can work on scipy sparse data; same as LSA\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.target_dim)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "        \n",
    "        \n",
    "        if self.trunc_svd:\n",
    "            self.trunc_svd_model = TruncatedSVD(n_components=self.target_dim) # Recommended 100\n",
    "            \n",
    "#             # TruncatedSVD may work w/o a dense matrix\n",
    "#             # Tf-idf returns a sparse one\n",
    "#             X = X.todense()\n",
    "            \n",
    "            # Run standard scaler - unsure if needed for svd\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.trunc_svd_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "        \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    r\"'quantit\\x89Û_https://t.co/64cyMG1lTG\",\n",
    "    r\"'quantitÛ_https://t.co/64cyMG1lTG\",\n",
    "    r\"'quantitû_https://t.co/64cymg1ltg\",\n",
    "    r\"quantitÛ_https://t.co/64cyMG1lTG\",\n",
    "    r\"quantitû_https://t.co/64cymg1ltg\",\n",
    "    r\"\\\\r\\\\n\",\n",
    "    r\"\\r\\n\",\n",
    "    r\"indiahttp://www.informationng.com/?p=309943\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"can 't\": \"cannot\",\n",
    "\"b/c\" : \"because\",\n",
    "\"cuz\": \"because\",\n",
    "\"kinda\": \"kind of\",\n",
    "\"hes\": \"he is\",\n",
    "\"shes\" : \"she is\",\n",
    "\"oh my god\": \"omg\",\n",
    "\"omfg\": \"omg\",\n",
    "\"didnt\": \"did not\",\n",
    "\"iûªm\": \"i am\",\n",
    "\"youûªve\": \"you have\",\n",
    "\"ûª\": \"'\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay\n",
      "okay\n",
      "welcome\n",
      "welcome\n",
      "rain\n",
      "rain\n",
      "\n",
      " \n",
      "['LINEENDING']\n",
      "Gave\n",
      "Gave\n",
      "storm\n",
      "storm\n",
      "weather\n",
      "weather\n"
     ]
    }
   ],
   "source": [
    "line_ending_pattern = re.compile(\"(\\r\\n|\\r|\\n)\", re.UNICODE)\n",
    "line_end_str = train_text_preprocessed[6272]\n",
    "\n",
    "test_doc = nlp(line_end_str)\n",
    "for token in test_doc:\n",
    "    print(token)\n",
    "    if len(re.findall(line_ending_pattern, token.text)) > 0:\n",
    "        print(['LINEENDING'])\n",
    "    else:\n",
    "        print(token.text)\n",
    "\n",
    "\n",
    "# handle_test = \"@asdfs @asdfklj asdfl;kjsl;fksdl;fj\"\n",
    "# no_handle = \"#asdf asdflkfdljk sdl;s. asdfpike!\"\n",
    "# # print(len(re.findall(user_handle_pattern, handle_test)))\n",
    "# handle_test2 = X_train.loc[180, 'text']\n",
    "# print(handle_test2)\n",
    "# for test_str in [handle_test2]:\n",
    "#     test_doc = nlp(test_str)\n",
    "#     for token in test_doc:\n",
    "#         print(token)\n",
    "#         if len(re.findall(user_handle_pattern, token.text)) > 0:\n",
    "#             print(['HANDLE'])\n",
    "#         else:\n",
    "#             print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Takes in a pandas series of strings, and returns the same.\n",
    "    \"\"\"\n",
    "    def __init__(self, expand_contractions=True, strip_url=True, strip_emails=True, strip_stopwords=True, strip_punct_flag=True, tag_numbers=True, lemmatize=True):\n",
    "        self.expand_contractions_flag = expand_contractions\n",
    "        self.strip_url_flag = strip_url\n",
    "        self.strip_emails_flag = strip_emails\n",
    "        self.strip_stopwords_flag = strip_stopwords\n",
    "        self.strip_punct_flag = strip_punct_flag\n",
    "        self.tag_numbers = tag_numbers\n",
    "        self.lemmatize = lemmatize\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@[A-Za-z0-9_]+[A-Za-z0-9-_]+\", re.UNICODE)\n",
    "        self.line_break_pattern = re.compile(\"(\\r\\n|\\r|\\n)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def expand_contractions(self, text, contraction_mapping=CONTRACTION_MAP):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "    \n",
    "#     def spacy_lemmatizer(self, token):\n",
    "#         return token.lemma_\n",
    "    \n",
    "    def spacy_token_preprocessing(self, text):\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            if token.like_url & self.strip_url_flag:\n",
    "                cleaned_tokens.append(\"[LINK]\")\n",
    "            elif token.like_email & self.strip_emails_flag:\n",
    "                cleaned_tokens.append(\"[EMAIL]\")\n",
    "            elif len(re.findall(self.user_handle_pattern, token.text)) > 0:\n",
    "                cleaned_tokens.append(\"[HANDLE]\")\n",
    "            elif token.is_punct & self.strip_punct_flag:\n",
    "                continue\n",
    "            elif token.like_num & self.tag_numbers:\n",
    "                cleaned_tokens.append(\"[NUM]\")\n",
    "            elif len(re.findall(self.line_break_pattern, token.text)) > 0:\n",
    "                continue\n",
    "            elif self.strip_stopwords_flag:\n",
    "                if (token.text.lower() in custom_stopwords) | token.is_stop:\n",
    "                    continue\n",
    "                elif self.lemmatize:\n",
    "                    cleaned_tokens.append(token.lemma_)\n",
    "                else:\n",
    "                    cleaned_tokens.append(token.text)\n",
    "            elif self.lemmatize:\n",
    "                cleaned_tokens.append(token.lemma_)\n",
    "            else:\n",
    "                cleaned_tokens.append(token.text)\n",
    "                \n",
    "        return \" \".join(cleaned_tokens)\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        X = raw_documents.copy()\n",
    "        \n",
    "        if self.expand_contractions_flag:\n",
    "            # Expand contractions in text\n",
    "            X = X.apply(self.expand_contractions)\n",
    "        \n",
    "        # All the remaining preprocessing\n",
    "        X = X.apply(self.spacy_token_preprocessing)\n",
    "            \n",
    "        return X\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = raw_documents.copy()\n",
    "        \n",
    "        if self.expand_contractions_flag:\n",
    "            # Expand contractions in text\n",
    "            X = X.apply(self.expand_contractions)\n",
    "            \n",
    "        # All the remaining preprocessing\n",
    "        X = X.apply(self.spacy_token_preprocessing)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-8912bdd77e69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext_processor_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_processor_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfs_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-59c237724d5b>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_contractions_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;31m# Expand contractions in text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_contractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;31m# All the remaining preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexl\\envs\\mlbase\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6877\u001b[0m         )\n\u001b[1;32m-> 6878\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6880\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"DataFrame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexl\\envs\\mlbase\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexl\\envs\\mlbase\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                 result = libreduction.compute_reduction(\n\u001b[1;32m--> 296\u001b[1;33m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.compute_reduction\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-59c237724d5b>\u001b[0m in \u001b[0;36mexpand_contractions\u001b[1;34m(self, text, contraction_mapping)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mexpanded_contraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mexpanded_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontractions_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpand_match\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mexpanded_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mexpanded_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "fs = FeatureSelector(['text'])\n",
    "fs_transformed = fs.fit_transform(X_train)\n",
    "print(type(fs_transformed))\n",
    "text_processor_pipe = TextPreprocessor()\n",
    "transformed = text_processor_pipe.fit_transform(fs_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180    @20skyhawkmm20 @traplord_29 @FREDOSANTANA300 @...\n",
       "181    If I get run over by an ambulance am I lucky? ...\n",
       "182    #news Twelve feared killed in Pakistani air am...\n",
       "183    http://t.co/7xGLah10zL Twelve feared killed in...\n",
       "184                   @TanSlash waiting for an ambulance\n",
       "185    @fouseyTUBE you ok? Need a ambulance. Hahahah ...\n",
       "186    AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE...\n",
       "187    Pakistan air ambulance helicopter crash kills ...\n",
       "188    @TheNissonian @RejectdCartoons nissan are you ...\n",
       "189    EMS1: NY EMTs petition for $17 per hour Û÷min...\n",
       "190    http://t.co/FCqmKFfflW Twelve feared killed in...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.loc[15:20, 'text']\n",
    "X_train.loc[180:190, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180    [HANDLE] [HANDLE] [HANDLE] [HANDLE] hella craz...\n",
       "181           run ambulance lucky justsaye randomthought\n",
       "182    news [NUM] fear kill pakistani air ambulance h...\n",
       "183    [LINK] [NUM] fear kill pakistani air ambulance...\n",
       "184                              [HANDLE] wait ambulance\n",
       "185       [HANDLE] ok need ambulance hahahah good [LINK]\n",
       "186    AMBULANCE SPRINTER AUTOMATIC FRONTLINE vehicle...\n",
       "187    pakistan air ambulance helicopter crash kill [...\n",
       "188    [HANDLE] [HANDLE] nissan ok need medical assis...\n",
       "189    ems1 NY emt petition $ [NUM] hour û÷minimum w...\n",
       "190    [LINK] [NUM] fear kill pakistani air ambulance...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processor.fit_transform(X_train.loc[180:190, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.7 s\n"
     ]
    }
   ],
   "source": [
    "%time train_text_preprocessed = text_processor.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deed reason earthquake allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near La Ronge Sask Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NUM] people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get send photo Ruby Alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>[NUM] giant crane hold bridge collapse nearby ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>[HANDLE] [HANDLE] control wild fire California...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>M1.94 01:04 UTC]?5 km S Volcano Hawaii [LINK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>Police investigate e bike collide car Little P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>late home raze Northern California Wildfire AB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                  deed reason earthquake allah forgive\n",
       "1                 forest fire near La Ronge Sask Canada\n",
       "2     resident ask shelter place notify officer evac...\n",
       "3     [NUM] people receive wildfire evacuation order...\n",
       "4     get send photo Ruby Alaska smoke wildfire pour...\n",
       "...                                                 ...\n",
       "7608  [NUM] giant crane hold bridge collapse nearby ...\n",
       "7609  [HANDLE] [HANDLE] control wild fire California...\n",
       "7610      M1.94 01:04 UTC]?5 km S Volcano Hawaii [LINK]\n",
       "7611  Police investigate e bike collide car Little P...\n",
       "7612  late home raze Northern California Wildfire AB...\n",
       "\n",
       "[7613 rows x 1 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>[link]</td>\n",
       "      <td>4723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>[handle]</td>\n",
       "      <td>2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[num]</td>\n",
       "      <td>2337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td></td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>like</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6720</th>\n",
       "      <td>ft.åêm.o.p.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6719</th>\n",
       "      <td>detonate&amp;amp;shot</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6718</th>\n",
       "      <td>succeed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>fiya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13469</th>\n",
       "      <td>symptom</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13470 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   index     0\n",
       "118               [link]  4723\n",
       "114             [handle]  2667\n",
       "21                 [num]  2337\n",
       "158                        568\n",
       "111                 like   394\n",
       "...                  ...   ...\n",
       "6720         ft.åêm.o.p.     1\n",
       "6719   detonate&amp;shot     1\n",
       "6718             succeed     1\n",
       "6717                fiya     1\n",
       "13469            symptom     1\n",
       "\n",
       "[13470 rows x 2 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_token_counts = get_token_counts(train_text_preprocessed)\n",
    "test_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_counts = get_token_counts(train_text_preprocessed)\n",
    "test_token_counts.to_csv(\"test_token_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'okay welcome rain \\r\\n Gave storm weather'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_preprocessed[6272]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the bow text training pipeline\n",
    "text_bow_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_bow', TextVectorizer(vec_method='bow'))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_raw_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalRawKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the lemmatized keyword categorical pipeline\n",
    "cat_lemma_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalLemmatizedKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer(pca=True, target_dim = 50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the bow text training with pca pipeline\n",
    "text_bow_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_bow', TextVectorizer(vec_method='bow', pca=True, target_dim=50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "test_text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_test_pipe', TextPreprocessor()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_raw_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_bow_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_bow_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_bow_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pca_50_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "test_text_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', test_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the full pipeline with the training and the model\n",
    "- The data transformation part of the pipeline that does TFIDF will return different number of features based on the data fed in\n",
    "\n",
    "# Solution: Separate the feature pipeline with the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_save_data(pipeline, description, save_dir=\"./\", keep_y = True, sparse_output = True):\n",
    "    # Keep target with features for h2o models\n",
    "    if keep_y:\n",
    "        train_processed = pipeline.fit_transform(X_train)\n",
    "        combined_train = np.concatenate([train_processed, y_train.reshape(-1, 1)], axis=1)\n",
    "    elif not keep_y:\n",
    "        train_processed = pipeline.fit_transform(X_train)\n",
    "        np.save(save_dir + description + '_y_train', y_train)\n",
    "    \n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    # Check that the dimensions are correct\n",
    "    if keep_y:\n",
    "        assert(combined_train.shape[1] == test_processed.shape[1] + 1), \"Shapes incorrect\"\n",
    "    else:\n",
    "        assert(combined_train.shape[1] == test_processed.shape[1]), \"Shapes incorrect\"\n",
    "    \n",
    "    if sparse_output:\n",
    "        sparse.save_npz(save_dir + description + '_train_sparse', sparse.csr_matrix(combined_train))\n",
    "        sparse.save_npz(save_dir + description + '_test_sparse', sparse.csr_matrix(test_processed))\n",
    "    elif not sparse_output:\n",
    "        np.save(save_dir + description + '_train_ndarray', combined_train)\n",
    "        np.save(save_dir + description + '_test_ndarray', test_processed)\n",
    "        \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.2s\n",
      "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 3) Processing text_test_pipe, total=   0.0s\n",
      "[Pipeline] ........ (step 3 of 3) Processing text_tfidf, total= 6.9min\n",
      "Done!\n",
      "Wall time: 9min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_and_save_data(test_text_pipeline, \"test_text_pipeline\", keep_y=True, sparse_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.2s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] .......... (step 2 of 2) Processing text_bow, total= 3.3min\n",
      "Done!\n",
      "CPU times: user 4min 41s, sys: 3.18 s, total: 4min 44s\n",
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_and_save_data(full_raw_keyword_bow_pipeline, \"full_raw_keyword_bow_pipeline_tt\", keep_y=True, sparse_output=False)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_ft\", keep_y=False, sparse_output=True)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_tf\", keep_y=True, sparse_output=False)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_ff\", keep_y=False, sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 3.0min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 2min 57s, sys: 1.64 s, total: 2min 59s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_processed = full_raw_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-deaf587a8514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_processed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_processed' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 972 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_processed = full_raw_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "sparse.save_npz('raw_keyword_categorical_X_train_20k_feat', sparse.csr_matrix(X_train_processed))\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "sparse.save_npz('raw_keyword_categorical_test_processed_20k_feat', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PCA'd (50 dims) training and test sets for the lemma and raw pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pipeline, name in zip([full_raw_keyword_pca_50_pipeline, full_lemma_keyword_pca_50_pipeline], ['full_raw_keyword_pca_50_pipeline', 'full_lemma_keyword_pca_50_pipeline']):\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    np.save(name + '_X_train', X_train_processed)\n",
    "    np.save(name + '_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_lemma_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_lemma_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "np.save('raw_keyword_categorical_X_train', X_train_processed)\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "np.save('raw_keyword_categorical_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "np.save('lemma_keyword_categorical_X_train_csr', sparse.csr_matrix(X_train_processed))\n",
    "np.save('lemma_keyword_categorical_y_train', y_train)\n",
    "# np.save('raw_keyword_categorical_y_train_csr', sparse.csr_matrix(y_train)) # Don't save as a sparse matrix, else you will need to reshape it later for training\n",
    "np.save('lemma_keyword_categorical_test_processed_csr', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed data\n",
    "- Save the output of the transform pipelines to save memory\n",
    "- Can save it raw (very large)\n",
    "- Or save as a sparse matrix\n",
    "- Do not save the target labels as a sparse as you'll have to reshape it from (1, n) to (n, ) later, and the space savings is probably very small\n",
    "- Note that there were some problems reading the sparse matrix into some sklearn models at training\n",
    "    - Will need to look into this problem more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000, # Try 4000...\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The full pipeline as a step in another pipeline with an estimator as the final step\n",
    "# full_pipeline_m = Pipeline(steps = [\n",
    "#     ('full_pipeline', full_pipeline),\n",
    "#     ('model', LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)) \n",
    "# ])\n",
    "\n",
    "# #Can call fit on it just like any other pipeline\n",
    "# full_pipeline_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df.copy().sample(1000, random_state=42)\n",
    "y_test = X_test.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test_processed = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict\n",
    "predicted = lrcv.predict(X_test_processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline_m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with tfidf, upper, lower text\n",
    "Logistic Regression Accuracy: 0.851  \n",
    "Logistic Regression Precision: 0.9287925696594427  \n",
    "Logistic Regression Recall: 0.704225352112676  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same as above but using the keyword column as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bumped up the LRCV iterations to 4000 due to non-convergence at iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upped cv to 10 using additional feature of num_hashtags in text\n",
    "# Model training takes under 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, vals in lrcv.scores_.items():\n",
    "    for idx, val in enumerate(vals):\n",
    "        print(idx)\n",
    "        print(val)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lrcv.predict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge predictions with correct ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = pd.read_csv(\"h2o_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, new_predictions['predict']]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.to_csv('test_preds_glm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
