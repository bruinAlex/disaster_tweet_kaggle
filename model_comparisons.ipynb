{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matcher for hashtags\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "                \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "        return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "- tf-idf\n",
    "- text is upper\n",
    "- text is lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.0s\n",
      "CPU times: user 1min 33s, sys: 1.43 s, total: 1min 34s\n",
      "Wall time: 1min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.2 s, sys: 1.05 s, total: 34.2 s\n",
      "Wall time: 15min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=10, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=4000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239874682916249"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/model_01.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "dump(lrcv, 'saved_models/model_01.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_01 = load('saved_models/model_01.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test predictions for kaggle scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 720 ms, total: 43 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict for model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = dict()\n",
    "model_results['model_01'] = {'best mean kfold score' : lrcv.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.80777\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "- tf-idf\n",
    "- text is upper\n",
    "- text is lower\n",
    "- include hashtag counts\n",
    "- include keywords as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_pipeline', cat_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 10min 20s, sys: 7.08 s, total: 10min 27s\n",
      "Wall time: 31min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=10, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=4000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv02 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv02.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/model_02.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv02.scores_[1].mean(axis=0).max()\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv02, 'saved_models/model_02.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_02 = load('saved_models/model_02.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 s, sys: 820 ms, total: 42.4 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv02.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_02'] = {'best mean kfold score' : lrcv02.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79243\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "- Only TF-IDF on tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('text_pipeline', text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "CPU times: user 2min 17s, sys: 2.45 s, total: 2min 20s\n",
      "Wall time: 16min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/model_03.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv03 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv03.fit(X_train_processed, y_train)\n",
    "\n",
    "lrcv03.scores_[1].mean(axis=0).max()\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv03, 'saved_models/model_03.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_03 = load('saved_models/model_03.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 788 ms, total: 43.7 s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv03.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243},\n",
       " 'model_03': {'best mean kfold score': 0.6228593210878816,\n",
       "  'kaggle submission score': 0.79959}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_03'] = {'best mean kfold score' : lrcv03.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79959\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "- TF-IDF\n",
    "- text is upper\n",
    "- text is lower\n",
    "- keywords as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_pipeline', cat_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.0s\n",
      "0.43962002382388465\n",
      "CPU times: user 6min 53s, sys: 5.71 s, total: 6min 59s\n",
      "Wall time: 19min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/model_04.joblib']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv04 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv04.fit(X_train_processed, y_train)\n",
    "\n",
    "print(lrcv04.scores_[1].mean(axis=0).max())\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv04, 'saved_models/model_04.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_03 = load('saved_models/model_03.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 s, sys: 848 ms, total: 42.8 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv04.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243},\n",
       " 'model_03': {'best mean kfold score': 0.6228593210878816,\n",
       "  'kaggle submission score': 0.79959},\n",
       " 'model_04': {'best mean kfold score': 0.43962002382388465,\n",
       "  'kaggle submission score': 0.79345}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_04'] = {'best mean kfold score' : lrcv04.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79345\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
