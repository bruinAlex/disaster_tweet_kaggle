{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Plan\n",
    "\n",
    "Based on previous data exploration, we'll start with the following:\n",
    "- Drop location\n",
    "- Convert keyword to a categorical\n",
    "- Vectorize tweet text using TF-IDF\n",
    "- Create categorical indicators from the text:\n",
    "    - all capitalized\n",
    "    - all lowercased\n",
    "    - count of hashtags\n",
    "    - count of user handles\n",
    "    - contains a date\n",
    "    - contains link\n",
    "    - contains timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import regex as re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = English() # This does not include certain features like lemmatization!\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # includes more features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create matcher for hashtags\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_counts(text_series, return_df = True, tokenizer=Tokenizer(nlp.vocab)):\n",
    "    \"\"\"\n",
    "    Helper function to get most common tokens.\n",
    "    \"\"\"\n",
    "    token_counts = Counter()\n",
    "    for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "        for token in doc:\n",
    "            # Skip url-like\n",
    "            if token.like_url:\n",
    "                continue\n",
    "            # Skip emails\n",
    "            if token.like_email:\n",
    "                continue\n",
    "            if token.text.lower() in custom_stopwords:\n",
    "                continue\n",
    "            token_counts[token.orth_.lower()] += 1 # Equivalently, token.text\n",
    "\n",
    "    if return_df:\n",
    "        token_counts_df = pd.DataFrame.from_dict(token_counts, orient='index').reset_index().sort_values(by=0, ascending=False)\n",
    "        return token_counts_df\n",
    "    else:\n",
    "        return token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text feature pipeline\n",
    "- Takes in the tweet text and returns various meta features about it\n",
    "- Does not tokenize or encode the text itself (taken care of in a separate pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self, use_count_hashtags=True, use_count_user_handles=True):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        self.use_count_hashtags = use_count_hashtags\n",
    "        self.use_count_user_handles = use_count_user_handles\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    def count_user_handles(self, obj):\n",
    "        user_handle_count = len(re.findall(self.user_handle_pattern, obj))\n",
    "        return user_handle_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        if self.count_hashtags:\n",
    "            # Count the number of hashtags in the text\n",
    "            X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        if self.count_user_handles:\n",
    "            # Count number of user handles\n",
    "            X['user_handle_count'] = X['text'].apply(self.count_user_handles)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalLemmatizedKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "        \n",
    "    def spacy_lemmatizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # There should only be one keyword (not removing %20 spaces)\n",
    "        if len(doc) > 1:\n",
    "            print('More than one token found; expecting single token')\n",
    "            \n",
    "        return doc[0].lemma_\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "\n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        # Convert the keywords to the lemmatized version\n",
    "        X['lemmatized_keyword'] = X['keyword'].apply(self.spacy_lemmatizer)\n",
    "        \n",
    "#         # Drop the keyword col\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X[['lemmatized_keyword']])\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalRawKeywordTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "#         # Instantiate OneHotEncoder\n",
    "#         ohe = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "#                                          drop='first',\n",
    "#                                          sparse=False) \n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "        # categorical_features = boolean mask for categorical columns\n",
    "        # sparse = False output an array not sparse matrix\n",
    "        \n",
    "#         # One-hot encode the keyword col\n",
    "#         X = pd.get_dummies(X, \n",
    "#                            columns=['keyword'], \n",
    "#                            drop_first=True, \n",
    "#                            dummy_na=True)\n",
    "\n",
    "#         # Drop original keyword col\n",
    "#         # The only thing remaining now will be the keyword labels\n",
    "#         X = X.drop('keyword', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self, pca=False, pca_n = None, remove_hashtag=True, remove_user_handle=True, remove_stop_words=True):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.pca_n = pca_n\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "        self.remove_user_handle = remove_user_handle\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))(@[A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "            \n",
    "        # Create a list of user handles\n",
    "        user_handles = re.findall(self.user_handle_pattern, doc.text)      \n",
    "        \n",
    "        # Convert spacy tokens to a list of string tokens\n",
    "        token_list = [t.text.lower() for t in doc if not t.is_punct | t.is_space]\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            token_list = [t for t in token_list if t not in stop_words]\n",
    "        \n",
    "        if self.remove_user_handle:\n",
    "            token_list = [t for t in token_list if t not in user_handles]\n",
    "        \n",
    "        if self.remove_hashtag:\n",
    "            token_list = [t.replace(\"#\", \"\") for t in token_list]\n",
    "            \n",
    "        return token_list\n",
    "\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "\n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.pca_n)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "            \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vec_method='tfidf', pca=False, pca_n = None, remove_hashtag=True, remove_user_handle=True, remove_stop_words=True):\n",
    "        self.vec_method = vec_method\n",
    "        if self.vec_method == 'tfidf':\n",
    "            self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        elif self.vec_method == 'bow':\n",
    "            self.bow_model = CountVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        self.pca = pca\n",
    "        self.pca_n = pca_n\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_hashtag = remove_hashtag\n",
    "        self.matcher = Matcher(nlp.vocab)\n",
    "        self.matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "        self.remove_user_handle = remove_user_handle\n",
    "        self.user_handle_pattern = re.compile(\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))(@[A-Za-z]+[A-Za-z0-9-_]+)\", re.UNICODE)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "            \n",
    "        # Create a list of user handles\n",
    "        user_handles = re.findall(self.user_handle_pattern, doc.text)      \n",
    "        \n",
    "        # Convert spacy tokens to a list of string tokens\n",
    "        token_list = [t.text.lower() for t in doc if not t.is_punct | t.is_space]\n",
    "        \n",
    "        if self.remove_stop_words:\n",
    "            token_list = [t for t in token_list if t not in stop_words]\n",
    "        \n",
    "        if self.remove_user_handle:\n",
    "            token_list = [t for t in token_list if t not in user_handles]\n",
    "        \n",
    "        if self.remove_hashtag:\n",
    "            token_list = [t.replace(\"#\", \"\") for t in token_list]\n",
    "            \n",
    "        return token_list\n",
    "\n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        if self.vec_method == 'tfidf':\n",
    "            X = self.tfidf_model.transform(raw_documents['text'])\n",
    "        elif self.vec_method == 'bow':\n",
    "            X = self.bow_model.transform(raw_documents['text'])\n",
    "            \n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.transform(X)\n",
    "\n",
    "            return X\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        if self.vec_method == 'tfidf':\n",
    "            X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "        elif self.vec_method == 'bow':\n",
    "            X = self.bow_model.fit_transform(raw_documents['text'])\n",
    "            \n",
    "        # If PCA\n",
    "        if self.pca:\n",
    "            self.pca_model = PCA(n_components=self.pca_n)\n",
    "            \n",
    "            # PCA requires a dense matrix\n",
    "            # Tf-idf returns a sparse one\n",
    "            X = X.todense()\n",
    "            \n",
    "            # Run standard scaler for PCA\n",
    "            X = preprocessing.StandardScaler().fit_transform(X)\n",
    "            \n",
    "            # Run PCA and return reduced array\n",
    "            X = self.pca_model.fit_transform(X)\n",
    "\n",
    "            return X\n",
    "            \n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = [\n",
    "    r\"'quantit\\x89Û_https://t.co/64cyMG1lTG\",\n",
    "    r\"'quantitÛ_https://t.co/64cyMG1lTG\",\n",
    "    r\"'quantitû_https://t.co/64cymg1ltg\",\n",
    "    r\"quantitÛ_https://t.co/64cyMG1lTG\",\n",
    "    r\"quantitû_https://t.co/64cymg1ltg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/bonus%20content/nlp%20proven%20approach/contractions.py\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"can 't\": \"cannot\",\n",
    "\"b/c\" : \"because\",\n",
    "\"cuz\": \"because\",\n",
    "\"kinda\": \"kind of\",\n",
    "\"hes\": \"he is\",\n",
    "\"shes\" : \"she is\",\n",
    "\"oh my god\": \"omg\",\n",
    "\"omfg\": \"omg\",\n",
    "\"didnt\": \"did not\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Takes in a pandas series of strings, and returns the same.\n",
    "    # TODO:\n",
    "    twitter handles\n",
    "    numeric (non-text) digits convert to digits\n",
    "    replace link with [LINK]\n",
    "    and digit with [DIGIT]\n",
    "    replace hashtag with [HASH]?\n",
    "    Would need to potentially store hashtags as a separate categorical\n",
    "    make single function that looks for all of the above, otherwise chaining multiple calls of nlp(text) will split the brackets on the placeholders above\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, expand_contractions=True, strip_url=True, strip_emails=True, strip_stopwords=True):\n",
    "        self.expand_contractions_flag = expand_contractions\n",
    "        self.strip_url_flag = strip_url\n",
    "        self.strip_emails_flag = strip_emails\n",
    "        self.strip_stopwords_flag = strip_stopwords\n",
    "        \n",
    "        \n",
    "    def expand_contractions(self, text, contraction_mapping=CONTRACTION_MAP):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                          flags=re.IGNORECASE|re.DOTALL)\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                    if contraction_mapping.get(match)\\\n",
    "                                    else contraction_mapping.get(match.lower())                       \n",
    "            expanded_contraction = first_char+expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def strip_url(self, text):\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Skip url-like\n",
    "            if token.like_url:\n",
    "                cleaned_tokens.append('[LINK]')\n",
    "#                 continue\n",
    "            else:\n",
    "                cleaned_tokens.append(token.text)\n",
    "        return \" \".join(cleaned_tokens)\n",
    "\n",
    "\n",
    "    def strip_emails(self, text):\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Skip email-like\n",
    "            if token.like_email:\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_tokens.append(token.text)\n",
    "        return \" \".join(cleaned_tokens)\n",
    "    \n",
    "    \n",
    "    def strip_stopwords(self, text):\n",
    "        doc = nlp(text)\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            # Skip stopwords\n",
    "            if (token.text.lower() in custom_stopwords) | token.is_stop:\n",
    "                continue\n",
    "            else:\n",
    "                cleaned_tokens.append(token.text)\n",
    "        return \" \".join(cleaned_tokens)\n",
    "    \n",
    "    \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        X = raw_documents.copy()\n",
    "        \n",
    "        if self.expand_contractions_flag:\n",
    "            # Expand contractions in text\n",
    "            X = X.apply(self.expand_contractions)\n",
    "            \n",
    "        if self.strip_url_flag:\n",
    "            # Remove stand-alone urls\n",
    "            X = X.apply(self.strip_url)\n",
    "            \n",
    "        if self.strip_stopwords_flag:\n",
    "            # Remove stopwords from spacy and custom lists\n",
    "            X = X.apply(self.strip_stopwords)\n",
    "            \n",
    "        return X\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = raw_documents.copy()\n",
    "        \n",
    "        if self.expand_contractions_flag:\n",
    "            # Expand contractions in text\n",
    "            X = X.apply(self.expand_contractions)\n",
    "            \n",
    "        if self.strip_url_flag:\n",
    "            # Remove stand-alone urls\n",
    "            X = X.apply(self.strip_url)\n",
    "        \n",
    "        if self.strip_stopwords_flag:\n",
    "            # Remove stopwords from spacy and custom lists\n",
    "            X = X.apply(self.strip_stopwords)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "# def get_token_counts(text_series, return_df = True, tokenizer=Tokenizer(nlp.vocab)):\n",
    "#     token_counts = Counter()\n",
    "#     for doc in tokenizer.pipe(texts, batch_size=50):\n",
    "#         for token in doc:\n",
    "#             # Skip url-like\n",
    "#             if token.like_url:\n",
    "#                 continue\n",
    "#             # Skip emails\n",
    "#             if token.like_email:\n",
    "#                 continue\n",
    "#             if token.text.lower() in custom_stopwords:\n",
    "#                 continue\n",
    "#             token_counts[token.orth_.lower()] += 1 # Equivalently, token.text\n",
    "\n",
    "#     if return_df:\n",
    "#         token_counts_df = pd.DataFrame.from_dict(token_counts, orient='index').reset_index().sort_values(by=0, ascending=False)\n",
    "#         return token_counts_df\n",
    "#     else:\n",
    "#         return token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180    @20skyhawkmm20 @traplord_29 @FREDOSANTANA300 @...\n",
       "181    If I get run over by an ambulance am I lucky? ...\n",
       "182    #news Twelve feared killed in Pakistani air am...\n",
       "183    http://t.co/7xGLah10zL Twelve feared killed in...\n",
       "184                   @TanSlash waiting for an ambulance\n",
       "185    @fouseyTUBE you ok? Need a ambulance. Hahahah ...\n",
       "186    AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE...\n",
       "187    Pakistan air ambulance helicopter crash kills ...\n",
       "188    @TheNissonian @RejectdCartoons nissan are you ...\n",
       "189    EMS1: NY EMTs petition for $17 per hour Û÷min...\n",
       "190    http://t.co/FCqmKFfflW Twelve feared killed in...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.loc[15:20, 'text']\n",
    "X_train.loc[180:190, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180    @20skyhawkmm20 @traplord_29 @FREDOSANTANA300 @...\n",
       "181    run ambulance lucky ? # justsaying # randomtho...\n",
       "182    # news feared killed Pakistani air ambulance h...\n",
       "183    [ LINK ] feared killed Pakistani air ambulance...\n",
       "184                          @TanSlash waiting ambulance\n",
       "185    @fouseyTUBE ok ? Need ambulance . Hahahah good...\n",
       "186    AMBULANCE SPRINTER AUTOMATIC FRONTLINE VEHICLE...\n",
       "187    Pakistan air ambulance helicopter crash kills ...\n",
       "188    @TheNissonian @RejectdCartoons nissan ok need ...\n",
       "189    EMS1 : NY EMTs petition $ 17 hour Û÷minimum w...\n",
       "190    [ LINK ] feared killed Pakistani air ambulance...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processor.fit_transform(X_train.loc[180:190, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the bow text training pipeline\n",
    "text_bow_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_bow', TextVectorizer(vec_method='bow'))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_raw_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalRawKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the lemmatized keyword categorical pipeline\n",
    "cat_lemma_keyword_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalLemmatizedKeywordTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer(pca=True, pca_n = 50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the bow text training with pca pipeline\n",
    "text_bow_pca_50_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_bow', TextVectorizer(vec_method='bow', pca=True, pca_n=50))\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "test_text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_test_pipe', TextPreprocessor()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_raw_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('text_pipeline', text_pca_50_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_bow_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_lemma_keyword_bow_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_lemma_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "full_raw_keyword_bow_pca_50_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', text_bow_pca_50_pipeline),\n",
    "                     ]\n",
    ")\n",
    "\n",
    "test_text_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_raw_keyword_pipeline', cat_raw_keyword_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "        ('text_pipeline', test_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can't run the full pipeline with the training and the model\n",
    "- The data transformation part of the pipeline that does TFIDF will return different number of features based on the data fed in\n",
    "\n",
    "# Solution: Separate the feature pipeline with the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_save_data(pipeline, description, save_dir=\"./\", keep_y = True, sparse_output = True):\n",
    "    # Keep target with features for h2o models\n",
    "    if keep_y:\n",
    "        train_processed = pipeline.fit_transform(X_train)\n",
    "        combined_train = np.concatenate([train_processed, y_train.reshape(-1, 1)], axis=1)\n",
    "    elif not keep_y:\n",
    "        train_processed = pipeline.fit_transform(X_train)\n",
    "        np.save(save_dir + description + '_y_train', y_train)\n",
    "    \n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    # Check that the dimensions are correct\n",
    "    if keep_y:\n",
    "        assert(combined_train.shape[1] == test_processed.shape[1] + 1), \"Shapes incorrect\"\n",
    "    else:\n",
    "        assert(combined_train.shape[1] == test_processed.shape[1]), \"Shapes incorrect\"\n",
    "    \n",
    "    if sparse_output:\n",
    "        sparse.save_npz(save_dir + description + '_train_sparse', sparse.csr_matrix(combined_train))\n",
    "        sparse.save_npz(save_dir + description + '_test_sparse', sparse.csr_matrix(test_processed))\n",
    "    elif not sparse_output:\n",
    "        np.save(save_dir + description + '_train_ndarray', combined_train)\n",
    "        np.save(save_dir + description + '_test_ndarray', test_processed)\n",
    "        \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.2s\n",
      "[Pipeline] ..... (step 1 of 3) Processing text_selector, total=   0.0s\n",
      "[Pipeline] .... (step 2 of 3) Processing text_test_pipe, total=   0.0s\n",
      "[Pipeline] ........ (step 3 of 3) Processing text_tfidf, total= 6.9min\n",
      "Done!\n",
      "Wall time: 9min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_and_save_data(test_text_pipeline, \"test_text_pipeline\", keep_y=True, sparse_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.2s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] .......... (step 2 of 2) Processing text_bow, total= 3.3min\n",
      "Done!\n",
      "CPU times: user 4min 41s, sys: 3.18 s, total: 4min 44s\n",
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform_and_save_data(full_raw_keyword_bow_pipeline, \"full_raw_keyword_bow_pipeline_tt\", keep_y=True, sparse_output=False)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_ft\", keep_y=False, sparse_output=True)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_tf\", keep_y=True, sparse_output=False)\n",
    "# transform_and_save_data(full_lemma_keyword_pca_50_pipeline, \"full_lemma_keyword_pca_50_pipeline_ff\", keep_y=False, sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 3.0min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 2min 57s, sys: 1.64 s, total: 2min 59s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_processed = full_raw_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-deaf587a8514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_processed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_processed' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 972 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_processed = full_raw_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "sparse.save_npz('raw_keyword_categorical_X_train_20k_feat', sparse.csr_matrix(X_train_processed))\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "sparse.save_npz('raw_keyword_categorical_test_processed_20k_feat', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PCA'd (50 dims) training and test sets for the lemma and raw pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pipeline, name in zip([full_raw_keyword_pca_50_pipeline, full_lemma_keyword_pca_50_pipeline], ['full_raw_keyword_pca_50_pipeline', 'full_lemma_keyword_pca_50_pipeline']):\n",
    "    X_train_processed = pipeline.fit_transform(X_train)\n",
    "    test_processed = pipeline.transform(test_df)\n",
    "    \n",
    "    np.save(name + '_X_train', X_train_processed)\n",
    "    np.save(name + '_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_lemma_keyword_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_lemma_keyword_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training and test numpy arrays\n",
    "np.save('raw_keyword_categorical_X_train', X_train_processed)\n",
    "np.save('raw_keyword_categorical_y_train', y_train)\n",
    "np.save('raw_keyword_categorical_test_processed', test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "np.save('lemma_keyword_categorical_X_train_csr', sparse.csr_matrix(X_train_processed))\n",
    "np.save('lemma_keyword_categorical_y_train', y_train)\n",
    "# np.save('raw_keyword_categorical_y_train_csr', sparse.csr_matrix(y_train)) # Don't save as a sparse matrix, else you will need to reshape it later for training\n",
    "np.save('lemma_keyword_categorical_test_processed_csr', sparse.csr_matrix(test_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving processed data\n",
    "- Save the output of the transform pipelines to save memory\n",
    "- Can save it raw (very large)\n",
    "- Or save as a sparse matrix\n",
    "- Do not save the target labels as a sparse as you'll have to reshape it from (1, n) to (n, ) later, and the space savings is probably very small\n",
    "- Note that there were some problems reading the sparse matrix into some sklearn models at training\n",
    "    - Will need to look into this problem more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000, # Try 4000...\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The full pipeline as a step in another pipeline with an estimator as the final step\n",
    "# full_pipeline_m = Pipeline(steps = [\n",
    "#     ('full_pipeline', full_pipeline),\n",
    "#     ('model', LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)) \n",
    "# ])\n",
    "\n",
    "# #Can call fit on it just like any other pipeline\n",
    "# full_pipeline_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = train_df.copy().sample(1000, random_state=42)\n",
    "y_test = X_test.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "X_test_processed = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict\n",
    "predicted = lrcv.predict(X_test_processed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_pipeline_m.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR with tfidf, upper, lower text\n",
    "Logistic Regression Accuracy: 0.851  \n",
    "Logistic Regression Precision: 0.9287925696594427  \n",
    "Logistic Regression Recall: 0.704225352112676  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same as above but using the keyword column as a categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bumped up the LRCV iterations to 4000 due to non-convergence at iterations=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upped cv to 10 using additional feature of num_hashtags in text\n",
    "# Model training takes under 40 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # # Predicting with a test dataset\n",
    "# predicted = pipe.predict(X_test)\n",
    "\n",
    "# # Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.f1_score(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, vals in lrcv.scores_.items():\n",
    "    for idx, val in enumerate(vals):\n",
    "        print(idx)\n",
    "        print(val)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrcv.scores_[1][0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Max auc_roc:', searchCV.scores_[1].mean(axis=0).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = lrcv.predict(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge predictions with correct ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test_df['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = pd.read_csv(\"h2o_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, new_predictions['predict']]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df.to_csv('test_preds_glm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
