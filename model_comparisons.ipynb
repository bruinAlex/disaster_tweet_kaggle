{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split#, cross_val_score\n",
    "from sklearn import metrics, preprocessing\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matcher for hashtags\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[ï¼ƒ#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "                \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer processes the keyword feature as a categorical\n",
    "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.ohe_model = preprocessing.OneHotEncoder(handle_unknown='error',\n",
    "                                         drop='first',\n",
    "                                         sparse=False)\n",
    "\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "    \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.transform(X)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Fill NaNs with \"None\"\n",
    "        # Missing values will cause the one-hot encoding to fail\n",
    "        X = X.fillna(\"none\")\n",
    "        \n",
    "        X = self.ohe_model.fit_transform(X)\n",
    "        \n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTfidfVectorizer(TfidfVectorizer):\n",
    "    def __init__(self):\n",
    "        self.tfidf_model = TfidfVectorizer(tokenizer=self.spacy_tokenizer)\n",
    "        \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "        return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "        \n",
    "    def transform(self, raw_documents):\n",
    "        X = self.tfidf_model.transform(raw_documents['text'])\n",
    "\n",
    "        return X.toarray() # Changes the scipy sparse array to a numpy matrix\n",
    "\n",
    "    \n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        X = self.tfidf_model.fit_transform(raw_documents['text'], y=y)\n",
    "\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "- tf-idf\n",
    "- text is upper\n",
    "- text is lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.copy()\n",
    "y_train = X_train.pop('target').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.0s\n",
      "CPU times: user 1min 36s, sys: 1.54 s, total: 1min 37s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.2 s, sys: 1.05 s, total: 34.2 s\n",
      "Wall time: 15min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=10, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=4000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lrcv =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "lrcv.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239874682916249"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv.scores_[1].mean(axis=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/model_01.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "dump(lrcv, 'saved_models/model_01.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_01 = load('saved_models/model_01.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get test predictions for kaggle scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.3 s, sys: 720 ms, total: 43 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dict for model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = dict()\n",
    "model_results['model_01'] = {'best mean kfold score' : lrcv.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.80777\n",
    "                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "- tf-idf\n",
    "- text is upper\n",
    "- text is lower\n",
    "- include hashtag counts\n",
    "- include keywords as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[ï¼ƒ#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def count_hashtags(self, obj):\n",
    "        hashtag_count = len(re.findall(self.hashtag_pattern, obj))\n",
    "        return hashtag_count\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Count the number of hashtags in the text\n",
    "        X['hashtag_count'] = X['text'].apply(self.count_hashtags)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "#              ('text_transformer', TextTokenizerTransformer()),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_pipeline', cat_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.1s\n",
      "CPU times: user 10min 20s, sys: 7.08 s, total: 10min 27s\n",
      "Wall time: 31min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=10, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=4000, multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                     random_state=42, refit=True, scoring='f1', solver='lbfgs',\n",
       "                     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv02 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv02.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/model_02.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrcv02.scores_[1].mean(axis=0).max()\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv02, 'saved_models/model_02.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_02 = load('saved_models/model_02.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.5 s, sys: 820 ms, total: 42.4 s\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv02.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_02'] = {'best mean kfold score' : lrcv02.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79243\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "- Only TF-IDF on tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('text_pipeline', text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "CPU times: user 2min 17s, sys: 2.45 s, total: 2min 20s\n",
      "Wall time: 16min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/model_03.joblib']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv03 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv03.fit(X_train_processed, y_train)\n",
    "\n",
    "lrcv03.scores_[1].mean(axis=0).max()\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv03, 'saved_models/model_03.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_03 = load('saved_models/model_03.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 788 ms, total: 43.7 s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv03.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_03.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243},\n",
       " 'model_03': {'best mean kfold score': 0.6228593210878816,\n",
       "  'kaggle submission score': 0.79959}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_03'] = {'best mean kfold score' : lrcv03.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79959\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4\n",
    "- TF-IDF\n",
    "- text is upper\n",
    "- text is lower\n",
    "- keywords as categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        self.hashtag_pattern = re.compile(\"(?:^|\\s)[ï¼ƒ#]{1}(\\w+)\", re.UNICODE)\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    \n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def fit_transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical text features\n",
    "cat_text_features = ['text']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# Categorical features for text pipeline\n",
    "cat_features = ['keyword']\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_text_pipeline = Pipeline(\n",
    "    steps = [('cat_text_selector', FeatureSelector(cat_text_features)),\n",
    "             ('cat_text_transformer', CategoricalTextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text training pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_tfidf', DenseTfidfVectorizer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the keyword categorical training pipeline\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalTransformer())\n",
    "            ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('cat_pipeline', cat_pipeline),\n",
    "        ('text_pipeline', text_pipeline),\n",
    "        ('cat_text_pipeline', cat_text_pipeline),\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing cat_selector, total=   0.0s\n",
      "[Pipeline] ... (step 2 of 2) Processing cat_transformer, total=   0.0s\n",
      "[Pipeline] ..... (step 1 of 2) Processing text_selector, total=   0.0s\n",
      "[Pipeline] ........ (step 2 of 2) Processing text_tfidf, total= 1.6min\n",
      "[Pipeline] . (step 1 of 2) Processing cat_text_selector, total=   0.0s\n",
      "[Pipeline]  (step 2 of 2) Processing cat_text_transformer, total=   0.0s\n",
      "0.43962002382388465\n",
      "CPU times: user 6min 53s, sys: 5.71 s, total: 6min 59s\n",
      "Wall time: 19min 35s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['saved_models/model_04.joblib']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Process text and categorical features\n",
    "X_train_processed = full_pipeline.fit_transform(X_train)\n",
    "\n",
    "lrcv04 =  LogisticRegressionCV(cv=10, \n",
    "                             max_iter = 4000,\n",
    "                             random_state=42, \n",
    "                             n_jobs=-1,\n",
    "                             scoring = 'f1',\n",
    "                            )\n",
    "\n",
    "lrcv04.fit(X_train_processed, y_train)\n",
    "\n",
    "print(lrcv04.scores_[1].mean(axis=0).max())\n",
    "\n",
    "# Save the model to disk\n",
    "dump(lrcv04, 'saved_models/model_04.joblib') \n",
    "\n",
    "# # To load it later:\n",
    "# model_03 = load('saved_models/model_03.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 s, sys: 848 ms, total: 42.8 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)\n",
    "\n",
    "test_predictions = lrcv04.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_01': {'best mean kfold score': 0.6239874682916249,\n",
       "  'kaggle submission score': 0.80777},\n",
       " 'model_02': {'best mean kfold score': 0.43945727482425345,\n",
       "  'kaggle submission score': 0.79243},\n",
       " 'model_03': {'best mean kfold score': 0.6228593210878816,\n",
       "  'kaggle submission score': 0.79959},\n",
       " 'model_04': {'best mean kfold score': 0.43962002382388465,\n",
       "  'kaggle submission score': 0.79345}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['model_04'] = {'best mean kfold score' : lrcv04.scores_[1].mean(axis=0).max(), \n",
    "                             'kaggle submission score' : 0.79345\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5: LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = LinearSVC(verbose = 1,\n",
    "                    random_state = 42,\n",
    "                   )\n",
    "\n",
    "distributions = dict(C=uniform(loc=0, scale=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomizedSearchCV(svc_clf, \n",
    "                         distributions, \n",
    "                         random_state=42,\n",
    "                         verbose=1,\n",
    "                         n_jobs=-1,\n",
    "                         scoring='f1',\n",
    "                         cv=10,\n",
    "#                          n_iter=60,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]CPU times: user 1.06 s, sys: 452 ms, total: 1.52 s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search = clf.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.23233444867279784}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([84.9352787 , 16.10157475,  1.54141743,  1.54591877,  1.20524662,\n",
       "         1.338851  ,  1.33740931,  1.47970021,  1.44681861,  1.37257459]),\n",
       " 'std_fit_time': array([12.92681729, 28.40206401,  0.19293621,  0.17539139,  0.27099235,\n",
       "         0.12867999,  0.20184244,  0.08188491,  0.12368427,  0.22038106]),\n",
       " 'mean_score_time': array([5.28051023, 0.03109226, 0.02909505, 0.02847872, 0.02032609,\n",
       "        0.02682462, 0.02478666, 0.03377442, 0.02543983, 0.02496336]),\n",
       " 'std_score_time': array([5.54990552, 0.0146152 , 0.00928098, 0.01251184, 0.00676768,\n",
       "        0.00779508, 0.01182587, 0.00839653, 0.00588926, 0.01017836]),\n",
       " 'param_C': masked_array(data=[1.49816047538945, 3.8028572256396647,\n",
       "                    2.9279757672456204, 2.3946339367881464,\n",
       "                    0.6240745617697461, 0.6239780813448106,\n",
       "                    0.23233444867279784, 3.4647045830997407,\n",
       "                    2.404460046972835, 2.832290311184182],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1.49816047538945},\n",
       "  {'C': 3.8028572256396647},\n",
       "  {'C': 2.9279757672456204},\n",
       "  {'C': 2.3946339367881464},\n",
       "  {'C': 0.6240745617697461},\n",
       "  {'C': 0.6239780813448106},\n",
       "  {'C': 0.23233444867279784},\n",
       "  {'C': 3.4647045830997407},\n",
       "  {'C': 2.404460046972835},\n",
       "  {'C': 2.832290311184182}],\n",
       " 'split0_test_score': array([0.63050847, 0.62207358, 0.62103506, 0.62311558, 0.65853659,\n",
       "        0.65853659, 0.64042934, 0.62207358, 0.62311558, 0.62103506]),\n",
       " 'split1_test_score': array([0.54453782, 0.52459016, 0.53530378, 0.53642384, 0.56491228,\n",
       "        0.56491228, 0.59439252, 0.52786885, 0.53642384, 0.53289474]),\n",
       " 'split2_test_score': array([0.51510334, 0.5       , 0.50314465, 0.50473186, 0.52980132,\n",
       "        0.52980132, 0.56206897, 0.5       , 0.50473186, 0.50314465]),\n",
       " 'split3_test_score': array([0.49371069, 0.4847561 , 0.48854962, 0.49155146, 0.51827243,\n",
       "        0.51827243, 0.54545455, 0.48549618, 0.49155146, 0.48623853]),\n",
       " 'split4_test_score': array([0.56962025, 0.54878049, 0.55246914, 0.56074766, 0.58670989,\n",
       "        0.58670989, 0.60767947, 0.55130168, 0.56074766, 0.55555556]),\n",
       " 'split5_test_score': array([0.57142857, 0.58544304, 0.576     , 0.57605178, 0.57859532,\n",
       "        0.57859532, 0.58782609, 0.58767773, 0.57605178, 0.57371795]),\n",
       " 'split6_test_score': array([0.59395973, 0.59637562, 0.60165289, 0.59468439, 0.61433447,\n",
       "        0.61433447, 0.61913043, 0.59834711, 0.59468439, 0.60165289]),\n",
       " 'split7_test_score': array([0.50273224, 0.4686941 , 0.48214286, 0.48028674, 0.51893939,\n",
       "        0.51893939, 0.54648956, 0.47142857, 0.48028674, 0.48214286]),\n",
       " 'split8_test_score': array([0.70464768, 0.68905109, 0.69106881, 0.69117647, 0.70998415,\n",
       "        0.70998415, 0.70915033, 0.69106881, 0.69117647, 0.69106881]),\n",
       " 'split9_test_score': array([0.71824818, 0.6955267 , 0.69956459, 0.70451237, 0.74328358,\n",
       "        0.74328358, 0.74223602, 0.69653179, 0.70451237, 0.7005814 ]),\n",
       " 'mean_test_score': array([0.5844497 , 0.57152909, 0.57509314, 0.57632821, 0.60233694,\n",
       "        0.60233694, 0.61548573, 0.57317943, 0.57632821, 0.57480324]),\n",
       " 'std_test_score': array([0.07500939, 0.07648155, 0.07427083, 0.07451147, 0.07499873,\n",
       "        0.07499873, 0.06266041, 0.07633902, 0.07451147, 0.07475874]),\n",
       " 'rank_test_score': array([ 4, 10,  7,  5,  2,  2,  1,  9,  5,  8], dtype=int32)}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6: Random Forest\n",
    "- Use pipeline from Model 1 which had highest score for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, \n",
    "                                random_state=42, \n",
    "                                verbose=51, \n",
    "                                n_jobs=1\n",
    "                               ) # Using all CPUs leads to memory crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "\n",
    "distributions = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomizedSearchCV(rf_clf, \n",
    "                         distributions, \n",
    "                         random_state=42,\n",
    "                         verbose=1,\n",
    "                         n_jobs=1,\n",
    "                         scoring='f1',\n",
    "                         cv=7,\n",
    "                         n_iter=60,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   18.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   19.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  4.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 48s, sys: 688 ms, total: 9min 49s\n",
      "Wall time: 9min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 600 out of 600 | elapsed:  5.0min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "search = clf.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   19.4s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   19.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   19.8s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.2s finished\n",
      "[Parallel(n_jobs=6)]: Using backend ThreadingBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=6)]: Done 100 out of 100 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "all_accuracies = cross_val_score(estimator=rf_clf, X=X_train_processed, y=y_train, cv=5, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54759358 0.45661157 0.52901998 0.49948187 0.65447898]\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374371951250232\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06610017992669956\n"
     ]
    }
   ],
   "source": [
    "print(all_accuracies.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_models/model_06.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "dump(rf_clf, 'saved_models/model_06.joblib') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.7 s, sys: 680 ms, total: 45.4 s\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "test_processed = full_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load it later:\n",
    "model_06 = load('saved_models/model_06.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=2)]: Done 1400 out of 1400 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.05 s, sys: 53.4 ms, total: 4.1 s\n",
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_predictions = model_06.predict(test_processed)\n",
    "test_id = test_df['id']\n",
    "test_predictions_df = pd.DataFrame([test_id, test_predictions]).T\n",
    "test_predictions_df.columns = ['id', 'target']\n",
    "test_predictions_df.to_csv('test_preds_06.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6c1e312410c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m model_results['model_06'] = {'best mean kfold score' : np.nan, \n\u001b[0;32m----> 2\u001b[0;31m                              \u001b[0;34m'kaggle submission score'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.79652\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                             }\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
     ]
    }
   ],
   "source": [
    "model_results['model_06'] = {'best mean kfold score' : np.nan, \n",
    "                             'kaggle submission score' : 0.79652\n",
    "                            }\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
