{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn FeatureUnion\n",
    "- Use custom transformers for feature engineering\n",
    "- Then merge the features horizontally for feeding into an ML classifier\n",
    "\n",
    "## FeatureUnion & Pipelines with Pandas\n",
    "https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import metrics\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matcher for hashtags\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selector transformer\n",
    "- Feed it the columns you want, and it returns a dataframe with just those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transformer that extracts columns passed as argument to its constructor \n",
    "class FeatureSelector( BaseEstimator, TransformerMixin ):\n",
    "    # Class Constructor \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names \n",
    "    \n",
    "    # Return self nothing else to do here    \n",
    "    def fit(self, X, y = None):\n",
    "        return self \n",
    "    \n",
    "    # Method that describes what we need this transformer to do\n",
    "    # This one pulls up the list of feature columns you pass in and returns just those columns\n",
    "    def transform(self, X, y = None):\n",
    "        return X[self.feature_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing transformer\n",
    "- Take in tweet text\n",
    "- Create features\n",
    "    - contains hashtag\n",
    "    - isupper\n",
    "    - islower\n",
    "    - has mispellings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the text pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns new categorical features\n",
    "class CategoricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer that takes in a string and returns some features\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    # Class constructor method that takes in a list of values as its argument\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    # Return self nothing else to do here\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "#     # Helper function to extract year from column 'dates' \n",
    "#     def get_year( self, obj ):\n",
    "#         return str(obj)[:4]\n",
    "    \n",
    "#     # Helper function to extract month from column 'dates'\n",
    "#     def get_month( self, obj ):\n",
    "#         return str(obj)[4:6]\n",
    "    \n",
    "#     # Helper function to extract day from column 'dates'\n",
    "#     def get_day(self, obj):\n",
    "#         return str(obj)[6:8]\n",
    "    \n",
    "#     # Helper function that converts values to Binary depending on input \n",
    "#     def create_binary(self, obj):\n",
    "#         if obj == 0:\n",
    "#             return 'No'\n",
    "#         else:\n",
    "#             return 'Yes'\n",
    "    \n",
    "    # Test helper func to just return the text in all lower case\n",
    "    def is_lower(self, obj):\n",
    "        if obj.islower():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def is_upper(self, obj):\n",
    "        if obj.isupper():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def spacy_tokenizer(self, obj):\n",
    "        doc = nlp(obj)\n",
    "\n",
    "        # Looks for hashtags\n",
    "        matches = matcher(doc)\n",
    "        spans = []\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "\n",
    "        for span in spans:\n",
    "            span.merge()\n",
    "\n",
    "        return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "\n",
    "\n",
    "    # Transformer method to take in strings from a dataframe and return some extra features\n",
    "    def transform(self, X , y = None):\n",
    "        # Copy the incoming df to prevent setting on copy errors\n",
    "        X = X.copy()\n",
    "        \n",
    "#         # Depending on constructor argument break dates column into specified units\n",
    "#         # using the helper functions written above \n",
    "#         for spec in self._use_dates:\n",
    "#             exec( \"X.loc[:,'{}'] = X['date'].apply(self.get_{})\".format( spec, spec ) )\n",
    "        \n",
    "#         # Drop unusable column \n",
    "#         X = X.drop('date', axis = 1 )\n",
    "\n",
    "#         # Convert these columns to binary for one-hot-encoding later\n",
    "#         X.loc[:,'waterfront'] = X['waterfront'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'view'] = X['view'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'yr_renovated'] = X['yr_renovated'].apply( self.create_binary )\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all lowercase\n",
    "        X['is_lower'] = X['text'].apply(self.is_lower)\n",
    "#         X.loc[:, 'is_lower'] = X['text'].apply(self.is_lower)\n",
    "        \n",
    "        # Return binary indicator of whether tweet is all uppercase\n",
    "        X['is_upper'] = X['text'].apply(self.is_upper)\n",
    "#         X.loc[:, 'is_upper'] = X['text'].apply(self.is_upper)\n",
    "        \n",
    "        # Embed text as a bag of words using tfidf\n",
    "        tfidf = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "    \n",
    "        # Drop original text col\n",
    "        # The only thing remaining now will be the lowercased text\n",
    "        X = X.drop('text', axis=1)\n",
    "        \n",
    "        # returns numpy array\n",
    "        return X.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categrical features to pass down the categorical pipeline \n",
    "# categorical_features = ['date', 'waterfront', 'view', 'yr_renovated']\n",
    "\n",
    "# #Numerical features to pass down the numerical pipeline \n",
    "# numerical_features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
    "#                 'condition', 'grade', 'sqft_basement', 'yr_built']\n",
    "\n",
    "# Text features for text pipeline\n",
    "text_features = ['text']\n",
    "\n",
    "# #Defining the steps in the categorical pipeline \n",
    "# categorical_pipeline = Pipeline( steps = [ ( 'cat_selector', FeatureSelector(categorical_features) ),\n",
    "                                  \n",
    "#                                   ( 'cat_transformer', CategoricalTransformer() ), \n",
    "                                  \n",
    "#                                   ( 'one_hot_encoder', OneHotEncoder( sparse = False ) ) ] )\n",
    "    \n",
    "# #Defining the steps in the numerical pipeline     \n",
    "# numerical_pipeline = Pipeline( steps = [ ( 'num_selector', FeatureSelector(numerical_features) ),\n",
    "                                  \n",
    "#                                   ( 'num_transformer', NumericalTransformer() ),\n",
    "                                  \n",
    "#                                   ('imputer', SimpleImputer(strategy = 'median') ),\n",
    "                                  \n",
    "#                                   ( 'std_scaler', StandardScaler() ) ] )\n",
    "\n",
    "# Define categorical pipeline\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [('cat_selector', FeatureSelector(cat_features)),\n",
    "             ('cat_transformer', CategoricalTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Define the text pipeline\n",
    "text_pipeline = Pipeline(\n",
    "    steps = [('text_selector', FeatureSelector(text_features)),\n",
    "             ('text_transformer', TextTransformer()),\n",
    "            ],\n",
    "    verbose = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Combining numerical and categorical piepline into one full big pipeline horizontally \n",
    "# #using FeatureUnion\n",
    "# full_pipeline = FeatureUnion( transformer_list = [ ( 'categorical_pipeline', categorical_pipeline ), \n",
    "                                                  \n",
    "#                                                   ( 'numerical_pipeline', numerical_pipeline ) ] )\n",
    "\n",
    "# Combine all our pipelines into a single one inside the FeatureUnion object\n",
    "# Right now we only have one pipeline which is our text one\n",
    "full_pipeline = FeatureUnion(\n",
    "    transformer_list=[('text_pipeline', text_pipeline),\n",
    "                      ('cat_pipeline', cat_pipeline)\n",
    "                     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a blank Tokenizer with just the English vocab\n",
    "# tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(string):\n",
    "    doc = nlp(string)\n",
    "#     return doc.text.split() # Disregards puncutation and contractions\n",
    "#     return [token.orth_ for token in doc] # This should work better, but isn't perfect. This also splits hashtags hash symbol and the word tag\n",
    "#     return [token.orth_ for token in doc if not token.is_punct | token.is_space]\n",
    "\n",
    "    # Looks for hashtags\n",
    "    matches = matcher(doc)\n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        spans.append(doc[start:end])\n",
    "\n",
    "\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "#     print([t.text for t in doc])\n",
    "#     return [t.text for t in doc if not t.is_punct | t.is_space]\n",
    "\n",
    "#     return string.isupper(), string.islower(), [t.text for t in doc if not t.is_punct | t.is_space]\n",
    "    return [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer_dataframe(df):\n",
    "    \"\"\"\n",
    "    Works on a dataframe rather than directly on the strings\n",
    "    Can't use this one for the sklearn pipeline\n",
    "    \"\"\"\n",
    "    doc = nlp(df['text'])\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    spans = []\n",
    "    for match_id, start, end in matches:\n",
    "        spans.append(doc[start:end])\n",
    "\n",
    "    for span in spans:\n",
    "        span.merge()\n",
    "\n",
    "    df['upper'] = df['text'].isupper()\n",
    "    df['lower'] = df['text'].islower()\n",
    "    df['token_list'] = [t.text.lower() for t in doc if t not in stop_words and not t.is_punct | t.is_space]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n",
    "\n",
    "doc = nlp(\"this is an #example of an awesome tweet with #manyhashtags #amazing\")\n",
    "matches = matcher(doc)\n",
    "spans = []\n",
    "for match_id, start, end in matches:\n",
    "    spans.append(doc[start:end])\n",
    "    \n",
    "    \n",
    "for span in spans:\n",
    "    span.merge()\n",
    "\n",
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer = spacy_tokenizer)\n",
    "# x = v.fit_transform(df['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfidf.fit_transform(train_df['text'].head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.todense()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(train_df['text'], train_df['target'], test_size=0.3, random_state=42)\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "\n",
    "# # Create pipeline using Bag of Words\n",
    "# pipe = Pipeline([\n",
    "#                 ('vectorizer', tfidf),\n",
    "#                  ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1).fit(train_df['text'], train_df['target'])\n",
    "classifier = LogisticRegressionCV(cv=5, random_state=42, n_jobs=-1)\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([\n",
    "                ('vectorizer', tfidf),\n",
    "                 ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model generation\n",
    "pipe.fit(train_df['text'],train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom transformer that breaks dates column into year, month and day into separate columns and\n",
    "# # converts certain features to binary \n",
    "# class CategoricalTransformer( BaseEstimator, TransformerMixin ):\n",
    "#     # Class constructor method that takes in a list of values as its argument\n",
    "#     def __init__(self, use_dates = ['year', 'month', 'day'] ):\n",
    "#         self._use_dates = use_dates\n",
    "        \n",
    "#     # Return self nothing else to do here\n",
    "#     def fit( self, X, y = None  ):\n",
    "#         return self\n",
    "\n",
    "#     # Helper function to extract year from column 'dates' \n",
    "#     def get_year( self, obj ):\n",
    "#         return str(obj)[:4]\n",
    "    \n",
    "#     # Helper function to extract month from column 'dates'\n",
    "#     def get_month( self, obj ):\n",
    "#         return str(obj)[4:6]\n",
    "    \n",
    "#     # Helper function to extract day from column 'dates'\n",
    "#     def get_day(self, obj):\n",
    "#         return str(obj)[6:8]\n",
    "    \n",
    "#     # Helper function that converts values to Binary depending on input \n",
    "#     def create_binary(self, obj):\n",
    "#         if obj == 0:\n",
    "#             return 'No'\n",
    "#         else:\n",
    "#             return 'Yes'\n",
    "    \n",
    "#     # Transformer method we wrote for this transformer \n",
    "#     def transform(self, X , y = None ):\n",
    "#         # Depending on constructor argument break dates column into specified units\n",
    "#         #using the helper functions written above \n",
    "#         for spec in self._use_dates:\n",
    "\n",
    "#         exec( \"X.loc[:,'{}'] = X['date'].apply(self.get_{})\".format( spec, spec ) )\n",
    "#         # Drop unusable column \n",
    "#         X = X.drop('date', axis = 1 )\n",
    "\n",
    "#         # Convert these columns to binary for one-hot-encoding later\n",
    "#         X.loc[:,'waterfront'] = X['waterfront'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'view'] = X['view'].apply( self.create_binary )\n",
    "\n",
    "#         X.loc[:,'yr_renovated'] = X['yr_renovated'].apply( self.create_binary )\n",
    "#         # returns numpy array\n",
    "#         return X.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
